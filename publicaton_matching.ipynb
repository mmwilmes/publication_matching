{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication matching\n",
    "- Identify publications that belong to the same scientific research field\n",
    "- based on: (1) text similarity and (2) references similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- generate target corpus (the publications for which we want to find matches)\n",
    "    - retrieve XML publications information from PubMed for specific search term\n",
    "    - restructure information into Article dataclass for easier processing\n",
    "    - pickle parsed articles (create break point in work flow)\n",
    "- generate general corpus (the publications from which we extract matches)\n",
    "- retrieve references for target corpus\n",
    "- determine text similarity \n",
    "    - text-frequency inverse document frequency (tf-idf) approach on titles and abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # construct file paths\n",
    "from Bio import Entrez    # query the NCBI API\n",
    "import configparser       # retrieve private credentials from file (which is ignored by git)\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from crossref.restful import Works    # query the Crossref REST API\n",
    "from crossref.restful import Works, Etiquette\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate target corpus\n",
    "- As example, we use publications of Madlen Vetter \n",
    "- Retrieve publication info from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"my_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search term for PupMed query\n",
    "search_term =  '(madlen vetter[author])'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_xml(search_term, pubmed_user, api_key, batch_size, file_to_open_batched, file_to_open_cleaned):\n",
    "    \"\"\"\n",
    "    Requirements:\n",
    "    - requires a search term\n",
    "    - a batch size that is downloaded from Entrez\n",
    "    - a file path to write out the data\n",
    "    Actions:\n",
    "    - calls the Entrez API\n",
    "    - prints the number of records for the search term\n",
    "    - saves webenv and querykey for subsequent searches\n",
    "    - posts the record IDs to the Entrez history server\n",
    "    - retrieves result in batches using the history server\n",
    "    - handles server timeouts and retries http calls\n",
    "    - deposits search_term at the end of the file\n",
    "    Output:\n",
    "    - prints progress along the way\n",
    "    - deposits batched file according to file_to_open_batched path object\n",
    "    - cleans repetitive XML headers (result of batching)\n",
    "    - deposits cleaned file according to file_to_open_cleaned path object\n",
    "    \"\"\"\n",
    "    Entrez.email = pubmed_user\n",
    "    apikey = pubmed_key\n",
    "\n",
    "    # test the PubMed waters, get the record count and save the history\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 30000, usehistory = \"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    count = int(record[\"Count\"])\n",
    "\n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "\n",
    "    # first identify the number of counts,\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = count)\n",
    "    record = Entrez.read(handle)\n",
    "\n",
    "    id_list = record[\"IdList\"]\n",
    "    assert count == len(id_list)\n",
    "    print(\"There are {} records for {}\".format(count, search_term))\n",
    "\n",
    "    post_xml = Entrez.epost(\"pubmed\", id = \",\".join(id_list))\n",
    "    search_results = Entrez.read(post_xml)\n",
    "\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "    # generate file handle for the path object\n",
    "    with file_to_open_batched.open(\"w\", encoding =\"utf-8\") as out_handle:\n",
    "        for start in range(0, count, batch_size):\n",
    "            end = min(count, start + batch_size)\n",
    "            print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "            attempt = 0\n",
    "            while attempt < 3:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    fetch_handle = Entrez.efetch(db = \"pubmed\", retmode = \"xml\",\n",
    "                                                     retstart = start, retmax = batch_size,\n",
    "                                                     webenv = webenv, query_key = query_key,\n",
    "                                                     api_key = apikey)\n",
    "                except HTTPError as err:\n",
    "                    if 500 <= err.code <= 599:\n",
    "                        print(\"Received error from server %s\" % err)\n",
    "                        print(\"Attempt %i of 3\" % attempt)\n",
    "                        time.sleep(15)\n",
    "                    else:\n",
    "                        raise\n",
    "            data = fetch_handle.read()\n",
    "            fetch_handle.close()\n",
    "            out_handle.write(data)\n",
    "\n",
    "    # deposit search term as comment at the end of the file\n",
    "    search_term_comment = \"\".join(['\\n<!--Generated by PubMed search term: ', search_term, \"-->\\n\"])\n",
    "\n",
    "    with file_to_open_batched.open(\"a\", encoding =\"utf-8\") as myfile:\n",
    "        myfile.write(search_term_comment)\n",
    "\n",
    "    # remove XML header lines that are artifacts of batch process\n",
    "    problems = ('<?xml version', \"<!DOCTYPE PubmedArticleSet PUBLIC\", \"<PubmedArticleSet\", \"</PubmedArticleSet\")\n",
    "    with file_to_open_batched.open(\"r\", encoding =\"utf-8\") as f:\n",
    "        with file_to_open_cleaned.open(\"w\", encoding =\"utf-8\") as out_file:\n",
    "            for i in range(10):\n",
    "                out_file.write(f.readline())\n",
    "            for line in f:\n",
    "                if not line.startswith(problems):\n",
    "                    out_file.write(line)\n",
    "            out_file.write(\"</PubmedArticleSet>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 records for (madlen vetter[author])\n",
      "Going to download record 1 to 4\n"
     ]
    }
   ],
   "source": [
    "# provide pubmed search term, pubmed user name, pubmed api key, \n",
    "# batch size, intermediate batch file path, and path object for final file\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build target corpus from PubMed XML information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataclass for articles\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "@dataclass\n",
    "# @dataclass_json\n",
    "class Article:\n",
    "    my_id: str = field(default = None)\n",
    "    doi: str = field(default = None)\n",
    "    pmid: str = field(default = None) # using a field allows to initiate without that info\n",
    "    authors: List[Any] = field(default_factory = list)\n",
    "    title: str = field(default = None)\n",
    "    abstract: str = field(default = None)\n",
    "    content: str = field(default = None)\n",
    "    journal: str = field(default = None)\n",
    "    year: int = field(default = 0)\n",
    "    references: List[Any] = field(default_factory = list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore XML structure\n",
    "#[elem.tag for elem in root.iter()]\n",
    "# articles = root.findall('.//PubmedArticle')\n",
    "# print(ET.tostring(articles[1]).decode(\"utf-8\"))\n",
    "# abstract = articles[9].find('.//Abstract')\n",
    "# print(ET.tostring(abstract, encoding='utf-8', method='xml').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_from_pubmed(root):\n",
    "    # root is an ElementTree element with the PubmedArticle tag\n",
    "    fields = {}\n",
    "    articleids = root.findall('.//ArticleId')\n",
    "    for Id in articleids:\n",
    "        # TODO isn't there a nicer way to do this?\n",
    "        if 'doi' in Id.attrib.values():\n",
    "            fields['doi'] = Id.text\n",
    "        if 'pubmed' in Id.attrib.values():\n",
    "            fields['pmid'] = Id.text\n",
    "    if 'doi' in fields:\n",
    "        fields['my_id'] = fields['doi']\n",
    "    elif 'pmid' in fields:\n",
    "        # Only use pmid for my_id if no doi\n",
    "        fields['my_id'] = fields['pmid']\n",
    "    authors = []\n",
    "    for surname in root.findall(\".//AuthorList/Author/LastName\"):\n",
    "        # TODO parse full name if needed\n",
    "        authors.append(surname.text)\n",
    "    fields['authors'] = authors\n",
    "    fields['title'] = root.findtext('.//ArticleTitle')\n",
    "    fields['journal'] = root.findtext('.//ISOAbbreviation')\n",
    "    fields['year'] =  root.findtext('.//JournalIssue/PubDate/Year')\n",
    "    abstract = root.find('.//Abstract')\n",
    "    if abstract:\n",
    "        fields['abstract'] = ET.tostring(abstract, encoding='utf-8', method='text').decode(\"utf-8\")\n",
    "    #if (doi AND title = title, authors=authors, journal=journal, year=year)\n",
    "    return Article(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    target_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle with processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(target_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle with processed publication information\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate general publication pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term =  'plants[mh] AND immun*[MH]'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"plant_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39198\n"
     ]
    }
   ],
   "source": [
    "# before retrieving anything, identify the number of counts\n",
    "Entrez.email = pubmed_user\n",
    "apikey = pubmed_key\n",
    "\n",
    "handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 500000, usehistory = \"y\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "webenv = record[\"WebEnv\"] \n",
    "query_key = record[\"QueryKey\"]\n",
    "\n",
    "id_list = record[\"IdList\"]\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Term': '\"plants\"[MeSH Terms]', 'Field': 'MeSH Terms', 'Count': '774600', 'Explode': 'Y'}, {'Term': 'immunity[MH]', 'Field': 'MH', 'Count': '335785', 'Explode': 'Y'}, {'Term': 'immunization[MH]', 'Field': 'MH', 'Count': '172778', 'Explode': 'Y'}, 'OR', {'Term': 'immunoassay[MH]', 'Field': 'MH', 'Count': '486189', 'Explode': 'Y'}, 'OR', {'Term': 'immunoblotting[MH]', 'Field': 'MH', 'Count': '203924', 'Explode': 'Y'}, 'OR', {'Term': 'immunochemistry[MH]', 'Field': 'MH', 'Count': '299867', 'Explode': 'Y'}, 'OR', {'Term': 'immunocompetence[MH]', 'Field': 'MH', 'Count': '7453', 'Explode': 'Y'}, 'OR', {'Term': 'immunoconglutinins[MH]', 'Field': 'MH', 'Count': '30', 'Explode': 'Y'}, 'OR', {'Term': 'immunoconjugates[MH]', 'Field': 'MH', 'Count': '11091', 'Explode': 'Y'}, 'OR', {'Term': 'immunodiffusion[MH]', 'Field': 'MH', 'Count': '46121', 'Explode': 'Y'}, 'OR', {'Term': 'immunoelectrophoresis[MH]', 'Field': 'MH', 'Count': '25514', 'Explode': 'Y'}, 'OR', {'Term': 'immunogenetics[MH]', 'Field': 'MH', 'Count': '2588', 'Explode': 'Y'}, 'OR', {'Term': 'immunoglobulins[MH]', 'Field': 'MH', 'Count': '877740', 'Explode': 'Y'}, 'OR', {'Term': 'immunohistochemistry[MH]', 'Field': 'MH', 'Count': '593313', 'Explode': 'Y'}, 'OR', {'Term': 'immunomodulation[MH]', 'Field': 'MH', 'Count': '314947', 'Explode': 'Y'}, 'OR', {'Term': 'immunophenotyping[MH]', 'Field': 'MH', 'Count': '28168', 'Explode': 'Y'}, 'OR', {'Term': 'immunophilins[MH]', 'Field': 'MH', 'Count': '5233', 'Explode': 'Y'}, 'OR', {'Term': 'immunoprecipitation[MH]', 'Field': 'MH', 'Count': '96399', 'Explode': 'Y'}, 'OR', {'Term': 'immunoproteins[MH]', 'Field': 'MH', 'Count': '945824', 'Explode': 'Y'}, 'OR', {'Term': 'immunosenescence[MH]', 'Field': 'MH', 'Count': '280', 'Explode': 'Y'}, 'OR', {'Term': 'immunosorbents[MH]', 'Field': 'MH', 'Count': '923', 'Explode': 'Y'}, 'OR', {'Term': 'immunosuppression[MH]', 'Field': 'MH', 'Count': '59399', 'Explode': 'Y'}, 'OR', {'Term': 'immunotherapy[MH]', 'Field': 'MH', 'Count': '271272', 'Explode': 'Y'}, 'OR', {'Term': 'immunotoxins[MH]', 'Field': 'MH', 'Count': '4418', 'Explode': 'Y'}, 'OR', {'Term': 'immunoturbidimetry[MH]', 'Field': 'MH', 'Count': '53', 'Explode': 'Y'}, 'OR', 'GROUP', 'AND']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve info on frequency of individual terms\n",
    "record['TranslationStack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pubmed_summary(webenv, query_key, apikey, numrec):\n",
    "    handle = Entrez.esummary(db=\"pubmed\", retmax = numrec, retmode=\"xml\", webenv = webenv, query_key = query_key, api_key = apikey)\n",
    "    records = Entrez.parse(handle)\n",
    "    # build a dict of dicts\n",
    "    data = {}\n",
    "    record_id = 0\n",
    "    for record in records:\n",
    "        # each record is a Python dictionary or list.\n",
    "        data[record_id] = data.get(record_id, {})\n",
    "        data[record_id].update(record)\n",
    "        record_id += 1       \n",
    "        print(record['Title']) #, record[\"AuthorList\"]\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins of peanut allergy-causing antibodies.\n",
      "Atypical Resistance Protein RPW8/HR Triggers Oligomerization of the NLR Immune Receptor RPP7 and Autoimmunity.\n",
      "Phenolic Amides with Immunomodulatory Activity from the Nonpolysaccharide Fraction of <i>Lycium barbarum</i> Fruits.\n",
      "Vaccarin hastens wound healing by promoting angiogenesis via activation of MAPK/ERK and PI3K/AKT signaling pathways in vivo.\n",
      "Cell Wall Membrane Fraction of <i>Chlorella sorokiniana</i> Enhances Host Antitumor Immunity and Inhibits Colon Carcinoma Growth in Mice.\n",
      "Identification of lncRNAs and their regulatory relationships with target genes and corresponding miRNAs in melon response to powdery mildew fungi.\n",
      "Genetic mapping using a wheat multi-founder population reveals a locus on chromosome 2A controlling resistance to both leaf and glume blotch caused by the necrotrophic fungal pathogen Parastagonospora nodorum.\n",
      "Identification of a Recessive Gene <i>PmQ</i> Conferring Resistance to Powdery Mildew in Wheat Landrace Qingxinmai Using BSR-Seq Analysis.\n",
      "PRR Cross-Talk Jump Starts Plant Immunity.\n",
      "A Rapid Survey of Avirulence Genes in Field Isolates of <i>Magnaporthe oryzae</i>.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the titles of some summary records to evaluate topical fit\n",
    "numrec = 10 # number of records\n",
    "get_pubmed_summary(webenv, query_key, pubmed_key, numrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39198 records for plants[mh] AND immun*[MH]\n",
      "Going to download record 1 to 5000\n",
      "Going to download record 5001 to 10000\n",
      "Going to download record 10001 to 15000\n",
      "Going to download record 15001 to 20000\n",
      "Going to download record 20001 to 25000\n",
      "Going to download record 25001 to 30000\n",
      "Going to download record 30001 to 35000\n",
      "Going to download record 35001 to 39198\n"
     ]
    }
   ],
   "source": [
    "# retrieve XML records for general publications\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root of general articles\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    general_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle with processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(general_articles, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add references using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossref etiquette\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "crossref_url = config.get(\"crossref\", \"url\")\n",
    "crossref_email = config.get(\"crossref\", \"email\")\n",
    "my_etiquette = Etiquette('Publication Matching', '0.1', crossref_url, crossref_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve crossref data\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: write functions and apply to target_articles and general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references = not_in_crossref = 0\n",
    "ref_articles = []\n",
    "for article in general_articles:\n",
    "    if article.doi:\n",
    "        ref_list = []\n",
    "        record = works.doi(article.doi)\n",
    "        if record:\n",
    "            if 'reference' in record:\n",
    "                for ref in record['reference']:\n",
    "                    title = ref.get('article-title', None)\n",
    "                    authors = ref.get('author', None)\n",
    "                    year = ref.get('year', None)\n",
    "                    journal = ref.get('journal-title', None)\n",
    "                    doi = ref.get('DOI', None)\n",
    "                    ref_list.append(Article(my_id=doi, doi=doi, title=title, authors=authors, year=year, journal=journal))\n",
    "                article.references = ref_list\n",
    "                ref_articles.append(article)\n",
    "            else:\n",
    "                no_references += 1\n",
    "        else: \n",
    "            not_in_crossref += 1\n",
    "print(\"Total number or articles: {}\".format(len(articles)))\n",
    "print(\"Not in crossref: {}\".format(not_in_crossref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle result articles with references\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve general articles with reference data\n",
    "# with XXXXfile_to_open_parsed.open(\"rb\") as infile:\n",
    "#     general_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_articles_dict = {}\n",
    "# for article in all_articles:\n",
    "#     all_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of articles and abstracts from publications, if abstract is sufficiently long\n",
    "# general_articles = []\n",
    "# general_abstracts = []\n",
    "# for article in all_articles:\n",
    "#     abstract = article.abstract or ''\n",
    "#     abstract = abstract.strip()\n",
    "#     if len(abstract) > 50:\n",
    "#         general_articles.append(article)\n",
    "#         general_abstracts.append(abstract)\n",
    "        \n",
    "# build a list with all target abstracts, and list of all target articles in same order\n",
    "# target_articles = []\n",
    "# target_abstracts = []\n",
    "# for article in target.values():\n",
    "#     target_articles.append(article)\n",
    "#     target_abstracts.append(article.abstract)\n",
    "\n",
    "# build a joint corpus with identifier\n",
    "# all_corpus = general_abstracts + target_abstracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
