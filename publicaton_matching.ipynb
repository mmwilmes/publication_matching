{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication matching\n",
    "- Identify publications that belong to the same scientific research field\n",
    "- based on: (1) text similarity and (2) references similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- generate target corpus (the publications for which we want to find matches)\n",
    "    - retrieve XML publications information from PubMed for specific search term\n",
    "    - restructure information into Article dataclass for easier processing\n",
    "    - pickle parsed articles (create break point in work flow)\n",
    "- retrieve references for target corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # construct file paths\n",
    "from Bio import Entrez    # query the NCBI API\n",
    "import configparser       # retrieve private credentials from file (which is ignored by git)\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from crossref.restful import Works    # query the Crossref REST API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate target corpus\n",
    "- As example, we use publications of Madlen Vetter \n",
    "- Retrieve publication info from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"my_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search term for PupMed query\n",
    "search_term =  '(madlen vetter[author])'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_xml(search_term, pubmed_user, api_key, batch_size, file_to_open_batched, file_to_open_cleaned):\n",
    "    \"\"\"\n",
    "    Requirements:\n",
    "    - requires a search term\n",
    "    - a batch size that is downloaded from Entrez\n",
    "    - a file path to write out the data\n",
    "    Actions:\n",
    "    - calls the Entrez API\n",
    "    - prints the number of records for the search term\n",
    "    - saves webenv and querykey for subsequent searches\n",
    "    - posts the record IDs to the Entrez history server\n",
    "    - retrieves result in batches using the history server\n",
    "    - handles server timeouts and retries http calls\n",
    "    - deposits search_term at the end of the file\n",
    "    Output:\n",
    "    - prints progress along the way\n",
    "    - deposits batched file according to file_to_open_batched path object\n",
    "    - cleans repetitive XML headers (result of batching)\n",
    "    - deposits cleaned file according to file_to_open_cleaned path object\n",
    "    \"\"\"\n",
    "    Entrez.email = pubmed_user\n",
    "    apikey = pubmed_key\n",
    "\n",
    "    # test the PubMed waters, get the record count and save the history\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 30000, usehistory = \"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    count = int(record[\"Count\"])\n",
    "\n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "\n",
    "    # first identify the number of counts,\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = count)\n",
    "    record = Entrez.read(handle)\n",
    "\n",
    "    id_list = record[\"IdList\"]\n",
    "    assert count == len(id_list)\n",
    "    print(\"There are {} records for {}\".format(count, search_term))\n",
    "\n",
    "    post_xml = Entrez.epost(\"pubmed\", id = \",\".join(id_list))\n",
    "    search_results = Entrez.read(post_xml)\n",
    "\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "    # generate file handle for the path object\n",
    "    with file_to_open_batched.open(\"w\", encoding =\"utf-8\") as out_handle:\n",
    "        for start in range(0, count, batch_size):\n",
    "            end = min(count, start + batch_size)\n",
    "            print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "            attempt = 0\n",
    "            while attempt < 3:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    fetch_handle = Entrez.efetch(db = \"pubmed\", retmode = \"xml\",\n",
    "                                                     retstart = start, retmax = batch_size,\n",
    "                                                     webenv = webenv, query_key = query_key,\n",
    "                                                     api_key = apikey)\n",
    "                except HTTPError as err:\n",
    "                    if 500 <= err.code <= 599:\n",
    "                        print(\"Received error from server %s\" % err)\n",
    "                        print(\"Attempt %i of 3\" % attempt)\n",
    "                        time.sleep(15)\n",
    "                    else:\n",
    "                        raise\n",
    "            data = fetch_handle.read()\n",
    "            fetch_handle.close()\n",
    "            out_handle.write(data)\n",
    "\n",
    "    # deposit search term as comment at the end of the file\n",
    "    search_term_comment = \"\".join(['\\n<!--Generated by PubMed search term: ', search_term, \"-->\\n\"])\n",
    "\n",
    "    with file_to_open_batched.open(\"a\", encoding =\"utf-8\") as myfile:\n",
    "        myfile.write(search_term_comment)\n",
    "\n",
    "    # remove XML header lines that are artifacts of batch process\n",
    "    problems = ('<?xml version', \"<!DOCTYPE PubmedArticleSet PUBLIC\", \"<PubmedArticleSet\", \"</PubmedArticleSet\")\n",
    "    with file_to_open_batched.open(\"r\", encoding =\"utf-8\") as f:\n",
    "        with file_to_open_cleaned.open(\"w\", encoding =\"utf-8\") as out_file:\n",
    "            for i in range(10):\n",
    "                out_file.write(f.readline())\n",
    "            for line in f:\n",
    "                if not line.startswith(problems):\n",
    "                    out_file.write(line)\n",
    "            out_file.write(\"</PubmedArticleSet>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 records for (madlen vetter[author])\n",
      "Going to download record 1 to 4\n"
     ]
    }
   ],
   "source": [
    "# provide pubmed search term, pubmed user name, pubmed api key, \n",
    "# batch size, intermediate batch file path, and path object for final file\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build target corpus from PubMed XML information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataclass for articles\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "@dataclass\n",
    "# @dataclass_json\n",
    "class Article:\n",
    "    my_id: str = field(default = None)\n",
    "    doi: str = field(default = None)\n",
    "    pmid: str = field(default = None) # using a field allows to initiate without that info\n",
    "    authors: List[Any] = field(default_factory = list)\n",
    "    title: str = field(default = None)\n",
    "    abstract: str = field(default = None)\n",
    "    content: str = field(default = None)\n",
    "    journal: str = field(default = None)\n",
    "    year: int = field(default = 0)\n",
    "    references: List[Any] = field(default_factory = list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore XML structure\n",
    "#[elem.tag for elem in root.iter()]\n",
    "# articles = root.findall('.//PubmedArticle')\n",
    "# print(ET.tostring(articles[1]).decode(\"utf-8\"))\n",
    "# abstract = articles[9].find('.//Abstract')\n",
    "# print(ET.tostring(abstract, encoding='utf-8', method='xml').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_from_pubmed(root):\n",
    "    # root is an ElementTree element with the PubmedArticle tag\n",
    "    fields = {}\n",
    "    articleids = root.findall('.//ArticleId')\n",
    "    for Id in articleids:\n",
    "        # TODO isn't there a nicer way to do this?\n",
    "        if 'doi' in Id.attrib.values():\n",
    "            fields['doi'] = Id.text\n",
    "        if 'pubmed' in Id.attrib.values():\n",
    "            fields['pmid'] = Id.text\n",
    "    if 'doi' in fields:\n",
    "        fields['my_id'] = fields['doi']\n",
    "    elif 'pmid' in fields:\n",
    "        # Only use pmid for my_id if no doi\n",
    "        fields['my_id'] = fields['pmid']\n",
    "    authors = []\n",
    "    for surname in root.findall(\".//AuthorList/Author/LastName\"):\n",
    "        # TODO parse full name if needed\n",
    "        authors.append(surname.text)\n",
    "    fields['authors'] = authors\n",
    "    fields['title'] = root.findtext('.//ArticleTitle')\n",
    "    fields['journal'] = root.findtext('.//ISOAbbreviation')\n",
    "    fields['year'] =  root.findtext('.//JournalIssue/PubDate/Year')\n",
    "    abstract = root.find('.//Abstract')\n",
    "    if abstract:\n",
    "        fields['abstract'] = ET.tostring(abstract, encoding='utf-8', method='text').decode(\"utf-8\")\n",
    "    #if (doi AND title = title, authors=authors, journal=journal, year=year)\n",
    "    return Article(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle with processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle with processed publication information\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add references using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossref etiquette\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "crossref_url = config.get(\"crossref\", \"url\")\n",
    "crossref_email = config.get(\"crossref\", \"email\")\n",
    "my_etiquette = Etiquette('Publication Matching', '0.1', crossref_url, crossref_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve crossref data\n",
    "from crossref.restful import Works, Etiquette\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number or articles: 4\n",
      "Not in crossref: 0\n"
     ]
    }
   ],
   "source": [
    "no_references = not_in_crossref = 0\n",
    "ref_articles = []\n",
    "for article in articles:\n",
    "    if article.doi:\n",
    "        ref_list = []\n",
    "        record = works.doi(article.doi)\n",
    "        if record:\n",
    "            if 'reference' in record:\n",
    "                for ref in record['reference']:\n",
    "                    title = ref.get('article-title', None)\n",
    "                    authors = ref.get('author', None)\n",
    "                    year = ref.get('year', None)\n",
    "                    journal = ref.get('journal-title', None)\n",
    "                    doi = ref.get('DOI', None)\n",
    "                    ref_list.append(Article(my_id=doi, doi=doi, title=title, authors=authors, year=year, journal=journal))\n",
    "                article.references = ref_list\n",
    "                ref_articles.append(article)\n",
    "            else:\n",
    "                no_references += 1\n",
    "        else: \n",
    "            not_in_crossref += 1\n",
    "print(\"Total number or articles: {}\".format(len(articles)))\n",
    "print(\"Not in crossref: {}\".format(not_in_crossref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle result articles with references\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve articles with reference data\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
