{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication matching\n",
    "- Identify publications that belong to the same scientific research field\n",
    "- based on: (1) text similarity and (2) references similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- generate target corpus (the publications for which we want to find matches)\n",
    "    - retrieve XML publications information from PubMed for specific search term\n",
    "    - restructure information into Article dataclass for easier processing\n",
    "    - pickle parsed articles (create break point in work flow)\n",
    "- generate general corpus (the publications from which we extract matches)\n",
    "- retrieve references for target corpus\n",
    "- determine text similarity \n",
    "    - text-frequency inverse document frequency (tf-idf) approach on titles and abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # construct file paths\n",
    "from Bio import Entrez    # query the NCBI API\n",
    "import configparser       # retrieve private credentials from file (which is ignored by git)\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from crossref.restful import Works    # query the Crossref REST API\n",
    "from crossref.restful import Works, Etiquette\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate target corpus\n",
    "- As example, we use publications of Madlen Vetter \n",
    "- Retrieve publication info from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"my_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create a folder to cache crossref results\n",
    "Path(main_dir / resultdir_string/ \"crossref_pickles\").mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search term for PupMed query\n",
    "search_term =  '(madlen vetter[author])'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_xml(search_term, pubmed_user, api_key, batch_size, file_to_open_batched, file_to_open_cleaned):\n",
    "    \"\"\"\n",
    "    Requirements:\n",
    "    - requires a search term\n",
    "    - a batch size that is downloaded from Entrez\n",
    "    - a file path to write out the data\n",
    "    Actions:\n",
    "    - calls the Entrez API\n",
    "    - prints the number of records for the search term\n",
    "    - saves webenv and querykey for subsequent searches\n",
    "    - posts the record IDs to the Entrez history server\n",
    "    - retrieves result in batches using the history server\n",
    "    - handles server timeouts and retries http calls\n",
    "    - deposits search_term at the end of the file\n",
    "    Output:\n",
    "    - prints progress along the way\n",
    "    - deposits batched file according to file_to_open_batched path object\n",
    "    - cleans repetitive XML headers (result of batching)\n",
    "    - deposits cleaned file according to file_to_open_cleaned path object\n",
    "    \"\"\"\n",
    "    Entrez.email = pubmed_user\n",
    "    apikey = pubmed_key\n",
    "\n",
    "    # test the PubMed waters, get the record count and save the history\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 30000, usehistory = \"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    count = int(record[\"Count\"])\n",
    "\n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "\n",
    "    # first identify the number of counts,\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = count)\n",
    "    record = Entrez.read(handle)\n",
    "\n",
    "    id_list = record[\"IdList\"]\n",
    "    assert count == len(id_list)\n",
    "    print(\"There are {} records for {}\".format(count, search_term))\n",
    "\n",
    "    post_xml = Entrez.epost(\"pubmed\", id = \",\".join(id_list))\n",
    "    search_results = Entrez.read(post_xml)\n",
    "\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "    # generate file handle for the path object\n",
    "    with file_to_open_batched.open(\"w\", encoding =\"utf-8\") as out_handle:\n",
    "        for start in range(0, count, batch_size):\n",
    "            end = min(count, start + batch_size)\n",
    "            print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "            attempt = 0\n",
    "            while attempt < 3:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    fetch_handle = Entrez.efetch(db = \"pubmed\", retmode = \"xml\",\n",
    "                                                     retstart = start, retmax = batch_size,\n",
    "                                                     webenv = webenv, query_key = query_key,\n",
    "                                                     api_key = apikey)\n",
    "                except HTTPError as err:\n",
    "                    if 500 <= err.code <= 599:\n",
    "                        print(\"Received error from server %s\" % err)\n",
    "                        print(\"Attempt %i of 3\" % attempt)\n",
    "                        time.sleep(15)\n",
    "                    else:\n",
    "                        raise\n",
    "            data = fetch_handle.read()\n",
    "            fetch_handle.close()\n",
    "            out_handle.write(data)\n",
    "\n",
    "    # deposit search term as comment at the end of the file\n",
    "    search_term_comment = \"\".join(['\\n<!--Generated by PubMed search term: ', search_term, \"-->\\n\"])\n",
    "\n",
    "    with file_to_open_batched.open(\"a\", encoding =\"utf-8\") as myfile:\n",
    "        myfile.write(search_term_comment)\n",
    "\n",
    "    # remove XML header lines that are artifacts of batch process\n",
    "    problems = ('<?xml version', \"<!DOCTYPE PubmedArticleSet PUBLIC\", \"<PubmedArticleSet\", \"</PubmedArticleSet\")\n",
    "    with file_to_open_batched.open(\"r\", encoding =\"utf-8\") as f:\n",
    "        with file_to_open_cleaned.open(\"w\", encoding =\"utf-8\") as out_file:\n",
    "            for i in range(10):\n",
    "                out_file.write(f.readline())\n",
    "            for line in f:\n",
    "                if not line.startswith(problems):\n",
    "                    out_file.write(line)\n",
    "            out_file.write(\"</PubmedArticleSet>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 records for (madlen vetter[author])\n",
      "Going to download record 1 to 4\n"
     ]
    }
   ],
   "source": [
    "# provide pubmed search term, pubmed user name, pubmed api key, \n",
    "# batch size, intermediate batch file path, and path object for final file\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build target corpus from PubMed XML information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataclass for articles\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "@dataclass\n",
    "# @dataclass_json\n",
    "class Article:\n",
    "    my_id: str = field(default = None)\n",
    "    doi: str = field(default = None)\n",
    "    pmid: str = field(default = None) # using a field allows to initiate without that info\n",
    "    authors: List[Any] = field(default_factory = list)\n",
    "    title: str = field(default = None)\n",
    "    abstract: str = field(default = None)\n",
    "    content: str = field(default = None)\n",
    "    journal: str = field(default = None)\n",
    "    year: int = field(default = 0)\n",
    "    references: List[Any] = field(default_factory = list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore XML structure\n",
    "#[elem.tag for elem in root.iter()]\n",
    "# articles = root.findall('.//PubmedArticle')\n",
    "# print(ET.tostring(articles[1]).decode(\"utf-8\"))\n",
    "# abstract = articles[9].find('.//Abstract')\n",
    "# print(ET.tostring(abstract, encoding='utf-8', method='xml').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_from_pubmed(root):\n",
    "    # root is an ElementTree element with the PubmedArticle tag\n",
    "    fields = {}\n",
    "    articleids = root.findall('.//ArticleId')\n",
    "    for Id in articleids:\n",
    "        # TODO isn't there a nicer way to do this?\n",
    "        if 'doi' in Id.attrib.values():\n",
    "            fields['doi'] = Id.text\n",
    "        if 'pubmed' in Id.attrib.values():\n",
    "            fields['pmid'] = Id.text\n",
    "    if 'doi' in fields:\n",
    "        fields['my_id'] = fields['doi']\n",
    "    elif 'pmid' in fields:\n",
    "        # Only use pmid for my_id if no doi\n",
    "        fields['my_id'] = fields['pmid']\n",
    "    authors = []\n",
    "    for surname in root.findall(\".//AuthorList/Author/LastName\"):\n",
    "        # TODO parse full name if needed\n",
    "        authors.append(surname.text)\n",
    "    fields['authors'] = authors\n",
    "    fields['title'] = root.findtext('.//ArticleTitle')\n",
    "    fields['journal'] = root.findtext('.//ISOAbbreviation')\n",
    "    fields['year'] =  root.findtext('.//JournalIssue/PubDate/Year')\n",
    "    abstract = root.find('.//Abstract')\n",
    "    if abstract:\n",
    "        fields['abstract'] = ET.tostring(abstract, encoding='utf-8', method='text').decode(\"utf-8\")\n",
    "    #if (doi AND title = title, authors=authors, journal=journal, year=year)\n",
    "    return Article(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    target_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle with processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(target_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle with processed publication information\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate general publication pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term =  'plants[mh] AND immun*[MH]'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"plant_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create a folder to cache crossref results\n",
    "Path(main_dir / resultdir_string/ \"crossref_pickles\").mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'\n",
    "directory_to_open_crossref = main_dir / resultdir_string/ \"crossref_pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39198\n"
     ]
    }
   ],
   "source": [
    "# before retrieving anything, identify the number of counts\n",
    "Entrez.email = pubmed_user\n",
    "apikey = pubmed_key\n",
    "\n",
    "handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 500000, usehistory = \"y\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "webenv = record[\"WebEnv\"] \n",
    "query_key = record[\"QueryKey\"]\n",
    "\n",
    "id_list = record[\"IdList\"]\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Term': '\"plants\"[MeSH Terms]', 'Field': 'MeSH Terms', 'Count': '774600', 'Explode': 'Y'}, {'Term': 'immunity[MH]', 'Field': 'MH', 'Count': '335785', 'Explode': 'Y'}, {'Term': 'immunization[MH]', 'Field': 'MH', 'Count': '172778', 'Explode': 'Y'}, 'OR', {'Term': 'immunoassay[MH]', 'Field': 'MH', 'Count': '486189', 'Explode': 'Y'}, 'OR', {'Term': 'immunoblotting[MH]', 'Field': 'MH', 'Count': '203924', 'Explode': 'Y'}, 'OR', {'Term': 'immunochemistry[MH]', 'Field': 'MH', 'Count': '299867', 'Explode': 'Y'}, 'OR', {'Term': 'immunocompetence[MH]', 'Field': 'MH', 'Count': '7453', 'Explode': 'Y'}, 'OR', {'Term': 'immunoconglutinins[MH]', 'Field': 'MH', 'Count': '30', 'Explode': 'Y'}, 'OR', {'Term': 'immunoconjugates[MH]', 'Field': 'MH', 'Count': '11091', 'Explode': 'Y'}, 'OR', {'Term': 'immunodiffusion[MH]', 'Field': 'MH', 'Count': '46121', 'Explode': 'Y'}, 'OR', {'Term': 'immunoelectrophoresis[MH]', 'Field': 'MH', 'Count': '25514', 'Explode': 'Y'}, 'OR', {'Term': 'immunogenetics[MH]', 'Field': 'MH', 'Count': '2588', 'Explode': 'Y'}, 'OR', {'Term': 'immunoglobulins[MH]', 'Field': 'MH', 'Count': '877740', 'Explode': 'Y'}, 'OR', {'Term': 'immunohistochemistry[MH]', 'Field': 'MH', 'Count': '593313', 'Explode': 'Y'}, 'OR', {'Term': 'immunomodulation[MH]', 'Field': 'MH', 'Count': '314947', 'Explode': 'Y'}, 'OR', {'Term': 'immunophenotyping[MH]', 'Field': 'MH', 'Count': '28168', 'Explode': 'Y'}, 'OR', {'Term': 'immunophilins[MH]', 'Field': 'MH', 'Count': '5233', 'Explode': 'Y'}, 'OR', {'Term': 'immunoprecipitation[MH]', 'Field': 'MH', 'Count': '96399', 'Explode': 'Y'}, 'OR', {'Term': 'immunoproteins[MH]', 'Field': 'MH', 'Count': '945824', 'Explode': 'Y'}, 'OR', {'Term': 'immunosenescence[MH]', 'Field': 'MH', 'Count': '280', 'Explode': 'Y'}, 'OR', {'Term': 'immunosorbents[MH]', 'Field': 'MH', 'Count': '923', 'Explode': 'Y'}, 'OR', {'Term': 'immunosuppression[MH]', 'Field': 'MH', 'Count': '59399', 'Explode': 'Y'}, 'OR', {'Term': 'immunotherapy[MH]', 'Field': 'MH', 'Count': '271272', 'Explode': 'Y'}, 'OR', {'Term': 'immunotoxins[MH]', 'Field': 'MH', 'Count': '4418', 'Explode': 'Y'}, 'OR', {'Term': 'immunoturbidimetry[MH]', 'Field': 'MH', 'Count': '53', 'Explode': 'Y'}, 'OR', 'GROUP', 'AND']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve info on frequency of individual terms\n",
    "record['TranslationStack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pubmed_summary(webenv, query_key, apikey, numrec):\n",
    "    handle = Entrez.esummary(db=\"pubmed\", retmax = numrec, retmode=\"xml\", webenv = webenv, query_key = query_key, api_key = apikey)\n",
    "    records = Entrez.parse(handle)\n",
    "    # build a dict of dicts\n",
    "    data = {}\n",
    "    record_id = 0\n",
    "    for record in records:\n",
    "        # each record is a Python dictionary or list.\n",
    "        data[record_id] = data.get(record_id, {})\n",
    "        data[record_id].update(record)\n",
    "        record_id += 1       \n",
    "        print(record['Title']) #, record[\"AuthorList\"]\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins of peanut allergy-causing antibodies.\n",
      "Atypical Resistance Protein RPW8/HR Triggers Oligomerization of the NLR Immune Receptor RPP7 and Autoimmunity.\n",
      "Phenolic Amides with Immunomodulatory Activity from the Nonpolysaccharide Fraction of <i>Lycium barbarum</i> Fruits.\n",
      "Vaccarin hastens wound healing by promoting angiogenesis via activation of MAPK/ERK and PI3K/AKT signaling pathways in vivo.\n",
      "Cell Wall Membrane Fraction of <i>Chlorella sorokiniana</i> Enhances Host Antitumor Immunity and Inhibits Colon Carcinoma Growth in Mice.\n",
      "Identification of lncRNAs and their regulatory relationships with target genes and corresponding miRNAs in melon response to powdery mildew fungi.\n",
      "Genetic mapping using a wheat multi-founder population reveals a locus on chromosome 2A controlling resistance to both leaf and glume blotch caused by the necrotrophic fungal pathogen Parastagonospora nodorum.\n",
      "Identification of a Recessive Gene <i>PmQ</i> Conferring Resistance to Powdery Mildew in Wheat Landrace Qingxinmai Using BSR-Seq Analysis.\n",
      "PRR Cross-Talk Jump Starts Plant Immunity.\n",
      "A Rapid Survey of Avirulence Genes in Field Isolates of <i>Magnaporthe oryzae</i>.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the titles of some summary records to evaluate topical fit\n",
    "numrec = 10 # number of records\n",
    "get_pubmed_summary(webenv, query_key, pubmed_key, numrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39198 records for plants[mh] AND immun*[MH]\n",
      "Going to download record 1 to 5000\n",
      "Going to download record 5001 to 10000\n",
      "Going to download record 10001 to 15000\n",
      "Going to download record 15001 to 20000\n",
      "Going to download record 20001 to 25000\n",
      "Going to download record 25001 to 30000\n",
      "Going to download record 30001 to 35000\n",
      "Going to download record 35001 to 39198\n"
     ]
    }
   ],
   "source": [
    "# retrieve XML records for general publications\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root of general articles\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    general_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle of processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(general_articles, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add references using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossref etiquette\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "crossref_url = config.get(\"crossref\", \"url\")\n",
    "crossref_email = config.get(\"crossref\", \"email\")\n",
    "my_etiquette = Etiquette('Publication Matching', '0.1', crossref_url, crossref_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve crossref data\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: write functions and apply to target_articles and general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references = not_in_crossref = 0\n",
    "ref_articles = []\n",
    "for article in general_articles:\n",
    "    if article.doi:\n",
    "        ref_list = []\n",
    "        record = works.doi(article.doi)\n",
    "        if record:\n",
    "            if 'reference' in record:\n",
    "                for ref in record['reference']:\n",
    "                    title = ref.get('article-title', None)\n",
    "                    authors = ref.get('author', None)\n",
    "                    year = ref.get('year', None)\n",
    "                    journal = ref.get('journal-title', None)\n",
    "                    doi = ref.get('DOI', None)\n",
    "                    ref_list.append(Article(my_id=doi, doi=doi, title=title, authors=authors, year=year, journal=journal))\n",
    "                article.references = ref_list\n",
    "                ref_articles.append(article)\n",
    "            else:\n",
    "                no_references += 1\n",
    "        else: \n",
    "            not_in_crossref += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle of processed publication information\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(ref_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17062"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17452 general articles.\n"
     ]
    }
   ],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 4 target articles.\n"
     ]
    }
   ],
   "source": [
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)\n",
    "print(\"Read in {} target articles.\".format(len(target_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target articles from the general article pool\n",
    "def remove_targets_from_general(target_articles, general_articles):\n",
    "    removed_targets = []\n",
    "    target_myids = set()\n",
    "\n",
    "    for article in target_articles:\n",
    "        target_myids.add(article.my_id)\n",
    "\n",
    "    for article in general_articles:\n",
    "        if article.my_id in target_myids:\n",
    "            removed_targets.append(article)\n",
    "            general_articles.remove(article)\n",
    "    for removed in removed_targets:\n",
    "        print(\"Removed target from pool: {}\".format(removed.title))\n",
    "    return general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed target from pool: Flagellin perception varies quantitatively in Arabidopsis thaliana and its relatives.\n"
     ]
    }
   ],
   "source": [
    "general_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 16755 articles from 17451 general articles.\n"
     ]
    }
   ],
   "source": [
    "# list of articles and abstracts from general publications, if abstract is sufficiently long\n",
    "pool_articles = []\n",
    "pool_abstracts = []\n",
    "for article in general_articles:\n",
    "    abstract = article.abstract or ''\n",
    "    abstract = abstract.strip()\n",
    "    if len(abstract) > 50:\n",
    "        pool_articles.append(article)\n",
    "        pool_abstracts.append(abstract)\n",
    "print(\"Retained {} articles from {} general articles.\".format(len(pool_articles), len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a list with all target abstracts, and list of all target articles in same order\n",
    "target_abstracts = []\n",
    "for article in target_articles:\n",
    "    target_abstracts.append(article.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept total of 16759 articles for NLP processing.\n"
     ]
    }
   ],
   "source": [
    "# build a joint corpus\n",
    "all_corpus = pool_abstracts + target_abstracts\n",
    "print(\"Kept total of {} articles for NLP processing.\".format(len(all_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary for easier look-up of matched articles\n",
    "pool_articles_dict = {}\n",
    "for article in pool_articles:\n",
    "    pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define STOP words\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_abstract(abstract):\n",
    "    # lower case and remove special characters/whitespaces\n",
    "    abstract = re.sub(r'[^a-zA-Z0-9\\s]', '', abstract, re.I|re.A)\n",
    "    abstract = abstract.lower()\n",
    "    abstract = abstract.strip()\n",
    "    # tokanize\n",
    "    tokens = nltk.word_tokenize(abstract)\n",
    "    # filter stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in STOP]\n",
    "    # re-create text from filtered tokens\n",
    "    abstract = ' '.join(filtered_tokens)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized 16759 articles.\n"
     ]
    }
   ],
   "source": [
    "normalize_corpus = np.vectorize(normalize_abstract)\n",
    "norm_corpus = normalize_corpus(all_corpus)\n",
    "print(\"Normalized {} articles.\".format(len(norm_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16759, 250389)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# We take uni-gram and bi-grams as our features and remove terms \n",
    "# that occur only in one document across the whole corpus.\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity comparison (Cosine similarity for pairwise document similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target and pool tfidf\n",
    "target_tfidf = tfidf_matrix[-len(target_abstracts):]\n",
    "pool_tfidf = tfidf_matrix[:-len(target_abstracts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run full matrix similarity for pool vs target\n",
    "sim = pool_tfidf @ target_tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save similarity matrix as numpy object (natural break-point in work flow)\n",
    "# np.save(\"doc_sim.npy\", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy object: \n",
    "# sim = np.load(\"doc_sim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix\n",
    "coo_sim = sim.tocoo(copy=False)\n",
    "pool_idx = coo_sim.row\n",
    "target_idx = coo_sim.col\n",
    "flat_sim = coo_sim.data\n",
    "#free up some memory\n",
    "del tfidf_matrix, target_tfidf, pool_tfidf, sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 22 articles above similarity threshold.\n"
     ]
    }
   ],
   "source": [
    "# filter for similarity threshold\n",
    "useful = np.argwhere(flat_sim > 0.13)\n",
    "filtered_pool_idx = pool_idx[useful].flatten()\n",
    "filtered_target_idx = target_idx[useful].flatten()\n",
    "filtered_flat_sim = flat_sim[useful].flatten()\n",
    "print(\"Identified {} articles above similarity threshold.\".format(len(useful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(filtered_flat_sim)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_matches has all matches in order\n",
    "filtered_pool_idx = np.array(filtered_pool_idx, dtype=int)\n",
    "filtered_target_idx = np.array(filtered_target_idx, dtype=int)\n",
    "sorted_matches = []\n",
    "for i in order:\n",
    "    match = (filtered_flat_sim[i], pool_articles[filtered_pool_idx[i]], target_articles[filtered_target_idx[i]])\n",
    "    sorted_matches.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency table (how many matches does each pool article have)\n",
    "from collections import Counter\n",
    "pool_hits = Counter(filtered_pool_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many articles have at least X matches?\n",
    "sum([1 for x in pool_hits.values() if x >= 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{726: 1,\n",
       " 1872: 1,\n",
       " 1889: 1,\n",
       " 2450: 1,\n",
       " 2566: 1,\n",
       " 2576: 2,\n",
       " 2648: 1,\n",
       " 3041: 1,\n",
       " 3456: 2,\n",
       " 6113: 1,\n",
       " 6193: 1,\n",
       " 6426: 1,\n",
       " 7532: 1,\n",
       " 8469: 1,\n",
       " 9043: 1,\n",
       " 9820: 1,\n",
       " 10361: 1,\n",
       " 11903: 1,\n",
       " 12078: 1,\n",
       " 13866: 1}"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the counter\n",
    "{x : pool_hits[x] for x in pool_hits if pool_hits[x] >= 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_matches = defaultdict(list) #keys are pool Article.my_id's, values are lists of matched target article obj\n",
    "for sim, pool, target in sorted_matches:\n",
    "    # create key; add similarity score; append a tuple that has matched target article and it\n",
    "    pool_matches[pool.my_id].append((sim, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out matches\n",
    "with open(\"./abstract_matches.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(pool_matches, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those with less than X matches\n",
    "# for my_id, match_list in list(pool_matches.items()):\n",
    "#     if len(match_list) < 2:\n",
    "#         pool_matches.pop(my_id)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_iter = iter(pool_matches.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool article:\n",
      "The receptor-like kinase SERK3/BAK1 is a central regulator of innate immunity in plants.\n",
      "10.1073/pnas.0705306104\n",
      "2007\n",
      "\n",
      "                In pathogen-associated molecular pattern (PAMP)-triggered immunity (PTI), plant cell surface receptors sense potential microbial pathogens by recognizing elicitors called PAMPs. Although diverse PAMPs trigger PTI through distinct receptors, the resulting intracellular responses overlap extensively. Despite this, a common component(s) linking signal perception with transduction remains unknown. In this study, we identify SOMATIC EMBRYOGENESIS RECEPTOR KINASE (SERK)3/brassinosteroid-associated kinase (BAK)1, a receptor-like kinase previously implicated in hormone signaling, as a component of plant PTI. In Arabidopsis thaliana, AtSERK3/BAK1 rapidly enters an elicitor-dependent complex with FLAGELLIN SENSING 2 (FLS2), the receptor for the bacterial PAMP flagellin and its peptide derivative flg22. In the absence of AtSERK3/BAK1, early flg22-dependent responses are greatly reduced in both A. thaliana and Nicotiana benthamiana. Furthermore, N. benthamiana Serk3/Bak1 is required for full responses to unrelated PAMPs and, importantly, for restriction of bacterial and oomycete infections. Thus, SERK3/BAK1 appears to integrate diverse perception events into downstream PAMP responses, leading to immunity against a range of invading microbes.\n",
      "            \n",
      "            \n",
      "\n",
      "Flagellin perception varies quantitatively in Arabidopsis thaliana and its relatives. 10.1093/molbev/mss011 2012 0.18717815917488811\n",
      "\n",
      "                Much is known about the evolution of plant immunity components directed against specific pathogen strains: They show pervasive functional variation and have the potential to coevolve with pathogen populations. However, plants are effectively protected against most microbes by generalist immunity components that detect conserved pathogen-associated molecular patterns (PAMPs) and control the onset of PAMP-triggered immunity. In Arabidopsis thaliana, the receptor kinase flagellin sensing 2 (FLS2) confers recognition of bacterial flagellin (flg22) and activates a manifold defense response. To decipher the evolution of this system, we performed functional assays across a large set of A. thaliana genotypes and Brassicaceae relatives. We reveal extensive variation in flg22 perception, most of which results from changes in protein abundance. The observed variation correlates with both the severity of elicited defense responses and bacterial proliferation. We analyzed nucleotide variation segregating at FLS2 in A. thaliana and detected a pattern of variation suggestive of the rapid fixation of a novel adaptive allele. However, our study also shows that evolution at the receptor locus alone does not explain the evolution of flagellin perception; instead, components common to pathways downstream of PAMP perception likely contribute to the observed quantitative variation. Within and among close relatives, PAMP perception evolves quantitatively, which contrasts with the changes in recognition typically associated with the evolution of R genes.\n",
      "            \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "my_id, matches = next(match_iter)\n",
    "print(\"Pool article:\")\n",
    "print(pool_articles_dict[my_id].title)\n",
    "print(pool_articles_dict[my_id].my_id)\n",
    "print(pool_articles_dict[my_id].year)\n",
    "print(pool_articles_dict[my_id].abstract)\n",
    "\n",
    "for sim, jm in matches:\n",
    "    print()\n",
    "    print(jm.title, jm.my_id, jm.year, sim)\n",
    "    print(jm.abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matching articles based on reference similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17452 general articles.\n"
     ]
    }
   ],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))\n",
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed target from pool: Flagellin perception varies quantitatively in Arabidopsis thaliana and its relatives.\n"
     ]
    }
   ],
   "source": [
    "# Remove targets from general pool\n",
    "general_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary for easier look-up of matched articles\n",
    "pool_articles_dict = {}\n",
    "for article in pool_articles:\n",
    "    pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_articles = []\n",
    "# for article in jovecorpus.values():\n",
    "#     # check if the string references have been converted (to article objects)\n",
    "#     if not any(isinstance(r, str) for r in article.references):\n",
    "#         jove_articles.append(article)\n",
    "\n",
    "# UNIQUE_ID = 100\n",
    "\n",
    "# def get_reference_token(article):\n",
    "#     global UNIQUE_ID\n",
    "#     if article.doi:\n",
    "#         return article.doi\n",
    "#     elif article.title:\n",
    "#         title = article.title.lower()\n",
    "#         return re.sub(r'[^a-z0-9]', '', title)\n",
    "#     else:\n",
    "#         # NOTE: if you want to try matching just on the understandable references,\n",
    "#         # you can instead return \"None\" here. (Expect more matches, but also more false positives)\n",
    "#         UNIQUE_ID += 1\n",
    "#         return \"LOCAL\" + str(UNIQUE_ID)\n",
    "    \n",
    "# def reference_tokenizer(article):\n",
    "#     tokens = []\n",
    "#     for ref in article.references:\n",
    "#         token = get_reference_token(ref)\n",
    "#         if token:\n",
    "#             tokens.append(token)\n",
    "#     return tokens\n",
    "        \n",
    "\n",
    "# n_jove = 0 # number of useful JoVE articles -> 5+ refs\n",
    "# ref_texts = []\n",
    "# ref_articles = []\n",
    "# for article in dro_articles:\n",
    "#     #check if this is a JoVE article and remove\n",
    "#     if article.journal and \"J Vis Exp\" in article.journal:\n",
    "#         continue\n",
    "#     tokens = reference_tokenizer(article)\n",
    "#     if tokens and len(tokens) > 5:\n",
    "#         ref_texts.append(tokens)\n",
    "#         ref_articles.append(article)\n",
    "# for article in jove_articles:\n",
    "#     tokens = reference_tokenizer(article)\n",
    "#     if tokens and len(tokens) > 5:\n",
    "#         n_jove += 1\n",
    "#         ref_texts.append(tokens)\n",
    "#         ref_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
