{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication matching\n",
    "- take a (small) text sample and identify matching publications (i.e., that belong to the same scientific field)\n",
    "- match based on: \\\n",
    "    (1) text similarity and \\\n",
    "    (2) references similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- generate target corpus (the publications for which we want to find matches)\n",
    "    - retrieve XML publications information from PubMed for specific search term\n",
    "    - restructure information into Article dataclass for easier processing\n",
    "    - pickle parsed articles (create break point in work flow)\n",
    "- generate general corpus (the pool of publications from which we later extract matches)\n",
    "- retrieve reference information for both target corpus and pool corpus\n",
    "- determine text similarity \n",
    "    - text-frequency inverse document frequency (tf-idf) approach on titles and abstracts\n",
    "- determine reference similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from pathlib import Path  # construct file paths\n",
    "import configparser       # retrieve private credentials from file (which is ignored by git)\n",
    "import pickle\n",
    "from Bio import Entrez    # query the NCBI API\n",
    "from crossref.restful import Works, Etiquette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Import custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ammend script folder to the Python path (run once --> adds path for duration of this session)\n",
    "sys.path.append('./scripts/')\n",
    "import pubmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pubmatch' from './scripts/pubmatch.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload pubmatch (re-run if library code changed during development)\n",
    "importlib.reload(pubmatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "# import numpy as np\n",
    "# import re\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate target corpus\n",
    "- As example, we use publications of Madlen Vetter \n",
    "- Retrieve publication info from PubMed\n",
    "- Build target corpus from PubMed XML results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search term for PupMed query\n",
    "search_term =  '(madlen vetter[author])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for result directory (e.g., according to PubMed search_term)\n",
    "result_dir_name = \"my_publications\"\n",
    "# create path object to result folder (adjust if not in current folder)\n",
    "result_dir = Path(\"./\" , result_dir_name)\n",
    "# mkdir result directory, if it does not exist\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "# create path object for clean XML records\n",
    "file_cleaned = result_dir / 'cleaned.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials for NCBI API (Entrez) that are stored in text file (not uploaded to git)\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 records for (madlen vetter[author])\n",
      "Going to download record 1 to 4\n"
     ]
    }
   ],
   "source": [
    "# provide pubmed search term, pubmed user name, pubmed api key, \n",
    "# batch size, intermediate batch file path, and path object for final file\n",
    "pubmatch.get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles = pubmatch.create_corpus(file_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate general corpus (pool against which target is matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search term for general pool of articles\n",
    "# tutorial on creating good search terms https://www.nlm.nih.gov/bsd/disted/pubmedtutorial/cover.html\n",
    "search_term =  'plants[MH] AND immunity[MH]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set up directory structure for general pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for result directory (e.g., according to PubMed search_term)\n",
    "result_dir_name = \"plant_publications\"\n",
    "# create path object to result folder (adjust if not in current folder)\n",
    "result_dir = Path(\"./\" , result_dir_name)\n",
    "# mkdir result directory, if it does not exist\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "# create path object for clean XML records\n",
    "file_cleaned = result_dir / 'cleaned.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in credentials for NCBI API (Entrez)\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Explore PubMed records for search term\n",
    "- Adjust search term if not sufficient or too many hits (--> insufficient RAM to process too many hits)\n",
    "- Aim for less than 50,000 records (capped at that number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13795\n"
     ]
    }
   ],
   "source": [
    "# before retrieving anything, identify the number of hits in PubMed\n",
    "Entrez.email = pubmed_user\n",
    "apikey = pubmed_key\n",
    "\n",
    "handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 500000, usehistory = \"y\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "webenv = record[\"WebEnv\"] \n",
    "query_key = record[\"QueryKey\"]\n",
    "\n",
    "id_list = record[\"IdList\"]\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Term': '\"plants\"[MeSH Terms]', 'Field': 'MeSH Terms', 'Count': '775888', 'Explode': 'Y'}, {'Term': '\"immunity\"[MeSH Terms]', 'Field': 'MeSH Terms', 'Count': '336173', 'Explode': 'Y'}, 'AND']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve info on frequency of individual terms\n",
    "record['TranslationStack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atypical Resistance Protein RPW8/HR Triggers Oligomerization of the NLR Immune Receptor RPP7 and Autoimmunity.\n",
      "Phenolic Amides with Immunomodulatory Activity from the Nonpolysaccharide Fraction of <i>Lycium barbarum</i> Fruits.\n",
      "Cell Wall Membrane Fraction of <i>Chlorella sorokiniana</i> Enhances Host Antitumor Immunity and Inhibits Colon Carcinoma Growth in Mice.\n",
      "Identification of lncRNAs and their regulatory relationships with target genes and corresponding miRNAs in melon response to powdery mildew fungi.\n",
      "Genetic mapping using a wheat multi-founder population reveals a locus on chromosome 2A controlling resistance to both leaf and glume blotch caused by the necrotrophic fungal pathogen Parastagonospora nodorum.\n",
      "Identification of a Recessive Gene <i>PmQ</i> Conferring Resistance to Powdery Mildew in Wheat Landrace Qingxinmai Using BSR-Seq Analysis.\n",
      "PRR Cross-Talk Jump Starts Plant Immunity.\n",
      "A Rapid Survey of Avirulence Genes in Field Isolates of <i>Magnaporthe oryzae</i>.\n",
      "Plant metabolism of nematode pheromones mediates plant-nematode interactions.\n",
      "Focus on Cell Biology of Virus-Plant and Virus-Vector Interactions.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the titles of some summary records to evaluate topical fit \n",
    "# (i.e., does the search term provide meaningfull PubMed records?)\n",
    "numrec = 10 # number of records\n",
    "pubmatch.get_pubmed_summary(webenv, query_key, pubmed_key, numrec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build the corpus of general publications (i.e., article pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve XML records for general publications\n",
    "pubmatch.get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus:\n",
    "#     search term\n",
    "#     xml file\n",
    "#     list of article objects\n",
    "    \n",
    "# for term, file in corpuses:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build corpus of general articles: generate from PubMed XML information or read from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_articles = pubmatch.create_corpus(file_to_open_cleaned, file_to_open_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pickles corpus of articles\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve reference information using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossref etiquette\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "crossref_url = config.get(\"crossref\", \"url\")\n",
    "crossref_email = config.get(\"crossref\", \"email\")\n",
    "my_etiquette = Etiquette('Publication Matching', '0.1', crossref_url, crossref_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up user agent for crossref API calls\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(general_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: write functions and apply to target_articles and general_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Retrieve references for target articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references = not_in_crossref = 0\n",
    "ref_articles = []\n",
    "for article in general_articles:\n",
    "    if article.doi:\n",
    "        ref_list = []\n",
    "        record = works.doi(article.doi)\n",
    "        if record:\n",
    "            if 'reference' in record:\n",
    "                for ref in record['reference']:\n",
    "                    title = ref.get('article-title', None)\n",
    "                    authors = ref.get('author', None)\n",
    "                    year = ref.get('year', None)\n",
    "                    journal = ref.get('journal-title', None)\n",
    "                    doi = ref.get('DOI', None)\n",
    "                    ref_list.append(Article(my_id=doi, doi=doi, title=title, authors=authors, year=year, journal=journal))\n",
    "                article.references = ref_list\n",
    "                ref_articles.append(article)\n",
    "            else:\n",
    "                no_references += 1\n",
    "        else: \n",
    "            not_in_crossref += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle of processed publication information\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(ref_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)\n",
    "print(\"Read in {} target articles.\".format(len(target_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target articles from the general article pool\n",
    "def remove_targets_from_general(target_articles, general_articles):\n",
    "    removed_targets = []\n",
    "    target_myids = set()\n",
    "\n",
    "    for article in target_articles:\n",
    "        target_myids.add(article.my_id)\n",
    "\n",
    "    for article in general_articles:\n",
    "        if article.my_id in target_myids:\n",
    "            removed_targets.append(article)\n",
    "            general_articles.remove(article)\n",
    "    for removed in removed_targets:\n",
    "        print(\"Removed target from pool: {}\".format(removed.title))\n",
    "    return general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of articles and abstracts from general publications, if abstract is sufficiently long\n",
    "pool_articles = []\n",
    "pool_abstracts = []\n",
    "for article in general_articles:\n",
    "    abstract = article.abstract or ''\n",
    "    abstract = abstract.strip()\n",
    "    if len(abstract) > 50:\n",
    "        pool_articles.append(article)\n",
    "        pool_abstracts.append(abstract)\n",
    "print(\"Retained {} articles from {} general articles.\".format(len(pool_articles), len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a list with all target abstracts, and list of all target articles in same order\n",
    "target_abstracts = []\n",
    "for article in target_articles:\n",
    "    target_abstracts.append(article.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a joint corpus\n",
    "all_corpus = pool_abstracts + target_abstracts\n",
    "print(\"Kept total of {} articles for NLP processing.\".format(len(all_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary for easier look-up of matched articles\n",
    "pool_articles_dict = {}\n",
    "for article in pool_articles:\n",
    "    pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define STOP words\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_abstract(abstract):\n",
    "    # lower case and remove special characters/whitespaces\n",
    "    abstract = re.sub(r'[^a-zA-Z0-9\\s]', '', abstract, re.I|re.A)\n",
    "    abstract = abstract.lower()\n",
    "    abstract = abstract.strip()\n",
    "    # tokanize\n",
    "    tokens = nltk.word_tokenize(abstract)\n",
    "    # filter stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in STOP]\n",
    "    # re-create text from filtered tokens\n",
    "    abstract = ' '.join(filtered_tokens)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_abstract)\n",
    "norm_corpus = normalize_corpus(all_corpus)\n",
    "print(\"Normalized {} articles.\".format(len(norm_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# We take uni-gram and bi-grams as our features and remove terms \n",
    "# that occur only in one document across the whole corpus.\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity comparison (Cosine similarity for pairwise document similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target and pool tfidf\n",
    "target_tfidf = tfidf_matrix[-len(target_abstracts):]\n",
    "pool_tfidf = tfidf_matrix[:-len(target_abstracts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run full matrix similarity for pool vs target\n",
    "sim = pool_tfidf @ target_tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save similarity matrix as numpy object (natural break-point in work flow)\n",
    "# np.save(\"doc_sim.npy\", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy object: \n",
    "# sim = np.load(\"doc_sim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix\n",
    "coo_sim = sim.tocoo(copy=False)\n",
    "pool_idx = coo_sim.row\n",
    "target_idx = coo_sim.col\n",
    "flat_sim = coo_sim.data\n",
    "#free up some memory\n",
    "del tfidf_matrix, target_tfidf, pool_tfidf, sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for similarity threshold\n",
    "useful = np.argwhere(flat_sim > 0.13)\n",
    "filtered_pool_idx = pool_idx[useful].flatten()\n",
    "filtered_target_idx = target_idx[useful].flatten()\n",
    "filtered_flat_sim = flat_sim[useful].flatten()\n",
    "print(\"Identified {} articles above similarity threshold.\".format(len(useful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(filtered_flat_sim)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_matches has all matches in order\n",
    "filtered_pool_idx = np.array(filtered_pool_idx, dtype=int)\n",
    "filtered_target_idx = np.array(filtered_target_idx, dtype=int)\n",
    "sorted_matches = []\n",
    "for i in order:\n",
    "    match = (filtered_flat_sim[i], pool_articles[filtered_pool_idx[i]], target_articles[filtered_target_idx[i]])\n",
    "    sorted_matches.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency table (how many matches does each pool article have)\n",
    "from collections import Counter\n",
    "pool_hits = Counter(filtered_pool_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles have at least X matches?\n",
    "sum([1 for x in pool_hits.values() if x >= 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the counter\n",
    "{x : pool_hits[x] for x in pool_hits if pool_hits[x] >= 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_matches = defaultdict(list) #keys are pool Article.my_id's, values are lists of matched target article obj\n",
    "for sim, pool, target in sorted_matches:\n",
    "    # create key; add similarity score; append a tuple that has matched target article and it\n",
    "    pool_matches[pool.my_id].append((sim, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out matches\n",
    "with open(\"./abstract_matches.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(pool_matches, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those with less than X matches\n",
    "# for my_id, match_list in list(pool_matches.items()):\n",
    "#     if len(match_list) < 2:\n",
    "#         pool_matches.pop(my_id)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_iter = iter(pool_matches.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_id, matches = next(match_iter)\n",
    "print(\"Pool article:\")\n",
    "print(pool_articles_dict[my_id].title)\n",
    "print(pool_articles_dict[my_id].my_id)\n",
    "print(pool_articles_dict[my_id].year)\n",
    "print(pool_articles_dict[my_id].abstract)\n",
    "\n",
    "for sim, jm in matches:\n",
    "    print()\n",
    "    print(jm.title, jm.my_id, jm.year, sim)\n",
    "    print(jm.abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matching articles based on reference similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))\n",
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)\n",
    "print(\"Read in {} target articles.\".format(len(target_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove targets from general pool\n",
    "pool_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_articles_dict = {}\n",
    "# # build a dictionary for easier look-up of matched articles\n",
    "# for article in pool_articles:\n",
    "#     pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles_dict = {}\n",
    "# build a dictionary for easier look-up of matched articles\n",
    "for article in target_articles:\n",
    "    target_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_temp = []\n",
    "for article in target_articles:\n",
    "    # check if the string references have been converted (to article objects)\n",
    "    if not any(isinstance(r, str) for r in article.references):\n",
    "        target_temp.append(article)\n",
    "print(\"{} of {} target articles have references.\".format(len(target_temp), len(target_articles)))\n",
    "# re-asign target_articles to remove target articles without reference information\n",
    "target_articles = target_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_ID = 100\n",
    "\n",
    "def get_reference_token(article):\n",
    "    global UNIQUE_ID\n",
    "    if article.doi:\n",
    "        return article.doi\n",
    "    elif article.title:\n",
    "        title = article.title.lower()\n",
    "        return re.sub(r'[^a-z0-9]', '', title)\n",
    "    else:\n",
    "        # NOTE: if you want to try matching just on the understandable references,\n",
    "        # you can instead return \"None\" here. (Expect more matches, but also more false positives)\n",
    "        UNIQUE_ID += 1\n",
    "        return \"LOCAL\" + str(UNIQUE_ID)\n",
    "\n",
    "def reference_tokenizer(article):\n",
    "    tokens = []\n",
    "    for ref in article.references:\n",
    "        token = get_reference_token(ref)\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_target = 0 # number of useful target articles (i.e., 5+ refs)\n",
    "ref_texts = []\n",
    "ref_articles = []\n",
    "for article in pool_articles:\n",
    "    tokens = reference_tokenizer(article)\n",
    "    if tokens and len(tokens) > 5:\n",
    "        ref_texts.append(tokens)\n",
    "        ref_articles.append(article)\n",
    "for article in target_articles:\n",
    "    tokens = reference_tokenizer(article)\n",
    "    if tokens and len(tokens) > 5:\n",
    "        n_target += 1\n",
    "        ref_texts.append(tokens)\n",
    "        ref_articles.append(article)\n",
    "print(\"Identified {} target articles with sufficient reference information.\".format(n_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering of reference information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# We take uni-gram and bi-grams as our features and remove terms \n",
    "# that occur only in one document across the whole corpus.         <- is that smart?\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_matrix = tf.fit_transform(ref_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cosine similarity matrix between pool and target articles\n",
    "pool_tfidf = tfidf_matrix[:-n_target]\n",
    "pool_articles = ref_articles[:-n_target]\n",
    "target_tfidf = tfidf_matrix[-n_target:]\n",
    "target_articles = ref_articles[-n_target:]\n",
    "sim = pool_tfidf @ target_tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all matching pairs of articles, in sorted order\n",
    "coo_sim = sim.tocoo(copy=False)\n",
    "pool_idx = coo_sim.row\n",
    "target_idx = coo_sim.col\n",
    "flat_sim = coo_sim.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory and order per similarity\n",
    "# del tfidf_matrix, target_tfidf, pool_tfidf, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust stringency of matches by filtering for flat_sim (similarity) \n",
    "useful = np.argwhere(flat_sim > 0.01)\n",
    "filtered_pool_idx = pool_idx[useful].flatten()\n",
    "filtered_target_idx = target_idx[useful].flatten()\n",
    "filtered_flat_sim = flat_sim[useful].flatten()\n",
    "\n",
    "from collections import Counter\n",
    "target_hits = Counter(filtered_target_idx)\n",
    "# for reference... number of target articles with XXX matches in pool\n",
    "print(sum([1 for x in target_hits.values() if x >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(filtered_flat_sim)[::-1]\n",
    "filtered_pool_idx = np.array(filtered_pool_idx, dtype=int)\n",
    "filtered_target_idx = np.array(filtered_target_idx, dtype=int)\n",
    "sorted_matches = []\n",
    "for i in order:\n",
    "    match = (filtered_flat_sim[i], target_articles[filtered_target_idx[i]], pool_articles[filtered_pool_idx[i]])\n",
    "    sorted_matches.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matches = defaultdict(list) #keys are target Article.my_id's, values are lists of matched pool article obj\n",
    "for sim, target, pool in sorted_matches:\n",
    "    # create key; add similarity score; append a tuple that has matched pool article and its sim score\n",
    "    target_matches[target.my_id].append((sim, pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out matches\n",
    "with open(\"./ref_matches.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(pool_matches, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if desired, remove those with less than X matches\n",
    "# for my_id, match_list in list(target_matches.items()):\n",
    "#     if len(match_list) < 4:\n",
    "#         target_matches.pop(my_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_iter = iter(target_matches.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through the results by re-running this cell multiple times\n",
    "my_id, matches = next(match_iter)\n",
    "print(\"Target article:\")\n",
    "print(target_articles_dict[my_id].title)\n",
    "print(target_articles_dict[my_id].my_id)\n",
    "print(target_articles_dict[my_id].year)\n",
    "print(target_articles_dict[my_id].abstract)\n",
    "\n",
    "for sim, pool_match in matches:\n",
    "    print()\n",
    "    print(\"Title: {} \\n my_id: {} \\n Year: {} \\n Similarity score: {} \\n Abstract: {}\".format(pool_match.title, pool_match.my_id, pool_match.year, sim, pool_match.abstract))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
