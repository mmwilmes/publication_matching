{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication matching\n",
    "- Identify publications that belong to the same scientific research field\n",
    "- based on: (1) text similarity and (2) references similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- generate target corpus (the publications for which we want to find matches)\n",
    "    - retrieve XML publications information from PubMed for specific search term\n",
    "    - restructure information into Article dataclass for easier processing\n",
    "    - pickle parsed articles (create break point in work flow)\n",
    "- generate general corpus (the publications from which we extract matches)\n",
    "- retrieve references for target corpus\n",
    "- determine text similarity \n",
    "    - text-frequency inverse document frequency (tf-idf) approach on titles and abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # construct file paths\n",
    "from Bio import Entrez    # query the NCBI API\n",
    "import configparser       # retrieve private credentials from file (which is ignored by git)\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from crossref.restful import Works    # query the Crossref REST API\n",
    "from crossref.restful import Works, Etiquette\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate target corpus\n",
    "- As example, we use publications of Madlen Vetter \n",
    "- Retrieve publication info from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"my_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create a folder to cache crossref results\n",
    "Path(main_dir / resultdir_string/ \"crossref_pickles\").mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search term for PupMed query\n",
    "search_term =  '(madlen vetter[author])'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_xml(search_term, pubmed_user, api_key, batch_size, file_to_open_batched, file_to_open_cleaned):\n",
    "    \"\"\"\n",
    "    Requirements:\n",
    "    - requires a search term\n",
    "    - a batch size that is downloaded from Entrez\n",
    "    - a file path to write out the data\n",
    "    Actions:\n",
    "    - calls the Entrez API\n",
    "    - prints the number of records for the search term\n",
    "    - saves webenv and querykey for subsequent searches\n",
    "    - posts the record IDs to the Entrez history server\n",
    "    - retrieves result in batches using the history server\n",
    "    - handles server timeouts and retries http calls\n",
    "    - deposits search_term at the end of the file\n",
    "    Output:\n",
    "    - prints progress along the way\n",
    "    - deposits batched file according to file_to_open_batched path object\n",
    "    - cleans repetitive XML headers (result of batching)\n",
    "    - deposits cleaned file according to file_to_open_cleaned path object\n",
    "    \"\"\"\n",
    "    Entrez.email = pubmed_user\n",
    "    apikey = pubmed_key\n",
    "\n",
    "    # test the PubMed waters, get the record count and save the history\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 30000, usehistory = \"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    count = int(record[\"Count\"])\n",
    "\n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "\n",
    "    # first identify the number of counts,\n",
    "    handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = count)\n",
    "    record = Entrez.read(handle)\n",
    "\n",
    "    id_list = record[\"IdList\"]\n",
    "    assert count == len(id_list)\n",
    "    print(\"There are {} records for {}\".format(count, search_term))\n",
    "\n",
    "    post_xml = Entrez.epost(\"pubmed\", id = \",\".join(id_list))\n",
    "    search_results = Entrez.read(post_xml)\n",
    "\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "    # generate file handle for the path object\n",
    "    with file_to_open_batched.open(\"w\", encoding =\"utf-8\") as out_handle:\n",
    "        for start in range(0, count, batch_size):\n",
    "            end = min(count, start + batch_size)\n",
    "            print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "            attempt = 0\n",
    "            while attempt < 3:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    fetch_handle = Entrez.efetch(db = \"pubmed\", retmode = \"xml\",\n",
    "                                                     retstart = start, retmax = batch_size,\n",
    "                                                     webenv = webenv, query_key = query_key,\n",
    "                                                     api_key = apikey)\n",
    "                except HTTPError as err:\n",
    "                    if 500 <= err.code <= 599:\n",
    "                        print(\"Received error from server %s\" % err)\n",
    "                        print(\"Attempt %i of 3\" % attempt)\n",
    "                        time.sleep(15)\n",
    "                    else:\n",
    "                        raise\n",
    "            data = fetch_handle.read()\n",
    "            fetch_handle.close()\n",
    "            out_handle.write(data)\n",
    "\n",
    "    # deposit search term as comment at the end of the file\n",
    "    search_term_comment = \"\".join(['\\n<!--Generated by PubMed search term: ', search_term, \"-->\\n\"])\n",
    "\n",
    "    with file_to_open_batched.open(\"a\", encoding =\"utf-8\") as myfile:\n",
    "        myfile.write(search_term_comment)\n",
    "\n",
    "    # remove XML header lines that are artifacts of batch process\n",
    "    problems = ('<?xml version', \"<!DOCTYPE PubmedArticleSet PUBLIC\", \"<PubmedArticleSet\", \"</PubmedArticleSet\")\n",
    "    with file_to_open_batched.open(\"r\", encoding =\"utf-8\") as f:\n",
    "        with file_to_open_cleaned.open(\"w\", encoding =\"utf-8\") as out_file:\n",
    "            for i in range(10):\n",
    "                out_file.write(f.readline())\n",
    "            for line in f:\n",
    "                if not line.startswith(problems):\n",
    "                    out_file.write(line)\n",
    "            out_file.write(\"</PubmedArticleSet>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide pubmed search term, pubmed user name, pubmed api key, \n",
    "# batch size, intermediate batch file path, and path object for final file\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build target corpus from PubMed XML information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataclass for articles\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "@dataclass\n",
    "# @dataclass_json\n",
    "class Article:\n",
    "    my_id: str = field(default = None)\n",
    "    doi: str = field(default = None)\n",
    "    pmid: str = field(default = None) # using a field allows to initiate without that info\n",
    "    authors: List[Any] = field(default_factory = list)\n",
    "    title: str = field(default = None)\n",
    "    abstract: str = field(default = None)\n",
    "    content: str = field(default = None)\n",
    "    journal: str = field(default = None)\n",
    "    year: int = field(default = 0)\n",
    "    references: List[Any] = field(default_factory = list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore XML structure\n",
    "#[elem.tag for elem in root.iter()]\n",
    "# articles = root.findall('.//PubmedArticle')\n",
    "# print(ET.tostring(articles[1]).decode(\"utf-8\"))\n",
    "# abstract = articles[9].find('.//Abstract')\n",
    "# print(ET.tostring(abstract, encoding='utf-8', method='xml').decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_from_pubmed(root):\n",
    "    # root is an ElementTree element with the PubmedArticle tag\n",
    "    fields = {}\n",
    "    articleids = root.findall('.//ArticleId')\n",
    "    for Id in articleids:\n",
    "        # TODO isn't there a nicer way to do this?\n",
    "        if 'doi' in Id.attrib.values():\n",
    "            fields['doi'] = Id.text\n",
    "        if 'pubmed' in Id.attrib.values():\n",
    "            fields['pmid'] = Id.text\n",
    "    if 'doi' in fields:\n",
    "        fields['my_id'] = fields['doi']\n",
    "    elif 'pmid' in fields:\n",
    "        # Only use pmid for my_id if no doi\n",
    "        fields['my_id'] = fields['pmid']\n",
    "    authors = []\n",
    "    for surname in root.findall(\".//AuthorList/Author/LastName\"):\n",
    "        # TODO parse full name if needed\n",
    "        authors.append(surname.text)\n",
    "    fields['authors'] = authors\n",
    "    fields['title'] = root.findtext('.//ArticleTitle')\n",
    "    fields['journal'] = root.findtext('.//ISOAbbreviation')\n",
    "    fields['year'] =  root.findtext('.//JournalIssue/PubDate/Year')\n",
    "    abstract = root.find('.//Abstract')\n",
    "    if abstract:\n",
    "        fields['abstract'] = ET.tostring(abstract, encoding='utf-8', method='text').decode(\"utf-8\")\n",
    "    #if (doi AND title = title, authors=authors, journal=journal, year=year)\n",
    "    return Article(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    target_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle with processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(target_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle with processed publication information\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate general publication pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term =  'plants[mh] AND immun*[MH]'\n",
    "# credentials for NCBI API (Entrez)\n",
    "# read credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "pubmed_user = config.get(\"pubmed\", \"user\")\n",
    "pubmed_key = config.get(\"pubmed\", \"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name directory and file according to search_term\n",
    "resultdir_string = \"plant_publications\"\n",
    "# define path\n",
    "main_dir = Path(\"./\")\n",
    "# mkdir result directory\n",
    "Path(main_dir / resultdir_string).mkdir(parents=True, exist_ok=True)\n",
    "# create a folder to cache crossref results\n",
    "Path(main_dir / resultdir_string/ \"crossref_pickles\").mkdir(parents=True, exist_ok=True)\n",
    "# create path object\n",
    "file_to_open_batched = main_dir / resultdir_string / 'batched.xml'\n",
    "file_to_open_cleaned = main_dir / resultdir_string / 'cleaned.xml'\n",
    "file_to_open_parsed = main_dir / resultdir_string / 'parsed_articles.pickle'\n",
    "directory_to_open_crossref = main_dir / resultdir_string/ \"crossref_pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before retrieving anything, identify the number of counts\n",
    "Entrez.email = pubmed_user\n",
    "apikey = pubmed_key\n",
    "\n",
    "handle = Entrez.esearch(db = \"pubmed\", term = search_term, retmax = 500000, usehistory = \"y\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "webenv = record[\"WebEnv\"] \n",
    "query_key = record[\"QueryKey\"]\n",
    "\n",
    "id_list = record[\"IdList\"]\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve info on frequency of individual terms\n",
    "record['TranslationStack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pubmed_summary(webenv, query_key, apikey, numrec):\n",
    "    handle = Entrez.esummary(db=\"pubmed\", retmax = numrec, retmode=\"xml\", webenv = webenv, query_key = query_key, api_key = apikey)\n",
    "    records = Entrez.parse(handle)\n",
    "    # build a dict of dicts\n",
    "    data = {}\n",
    "    record_id = 0\n",
    "    for record in records:\n",
    "        # each record is a Python dictionary or list.\n",
    "        data[record_id] = data.get(record_id, {})\n",
    "        data[record_id].update(record)\n",
    "        record_id += 1       \n",
    "        print(record['Title']) #, record[\"AuthorList\"]\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the titles of some summary records to evaluate topical fit\n",
    "numrec = 10 # number of records\n",
    "get_pubmed_summary(webenv, query_key, pubmed_key, numrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve XML records for general publications\n",
    "get_clean_xml(search_term, pubmed_user, pubmed_key, 5000, file_to_open_batched, file_to_open_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XML file and find root of general articles\n",
    "with file_to_open_cleaned.open(\"r\", encoding =\"utf-8\") as infile:\n",
    "    tree = ET.parse(infile)\n",
    "    root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_articles = []\n",
    "for article in root.findall('.//PubmedArticle'):\n",
    "    parsed = article_from_pubmed(article)\n",
    "    general_articles.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle of processed publication information (natural break point in work flow)\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(general_articles, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add references using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossref etiquette\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../credentials/publication_matching_creds.txt\")\n",
    "crossref_url = config.get(\"crossref\", \"url\")\n",
    "crossref_email = config.get(\"crossref\", \"email\")\n",
    "my_etiquette = Etiquette('Publication Matching', '0.1', crossref_url, crossref_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve crossref data\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: write functions and apply to target_articles and general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickle\n",
    "with file_to_open_parsed.open(\"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references = not_in_crossref = 0\n",
    "ref_articles = []\n",
    "for article in general_articles:\n",
    "    if article.doi:\n",
    "        ref_list = []\n",
    "        record = works.doi(article.doi)\n",
    "        if record:\n",
    "            if 'reference' in record:\n",
    "                for ref in record['reference']:\n",
    "                    title = ref.get('article-title', None)\n",
    "                    authors = ref.get('author', None)\n",
    "                    year = ref.get('year', None)\n",
    "                    journal = ref.get('journal-title', None)\n",
    "                    doi = ref.get('DOI', None)\n",
    "                    ref_list.append(Article(my_id=doi, doi=doi, title=title, authors=authors, year=year, journal=journal))\n",
    "                article.references = ref_list\n",
    "                ref_articles.append(article)\n",
    "            else:\n",
    "                no_references += 1\n",
    "        else: \n",
    "            not_in_crossref += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out pickle of processed publication information\n",
    "with file_to_open_parsed.open(\"wb\") as outfile:\n",
    "    pickle.dump(ref_articles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)\n",
    "print(\"Read in {} target articles.\".format(len(target_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target articles from the general article pool\n",
    "def remove_targets_from_general(target_articles, general_articles):\n",
    "    removed_targets = []\n",
    "    target_myids = set()\n",
    "\n",
    "    for article in target_articles:\n",
    "        target_myids.add(article.my_id)\n",
    "\n",
    "    for article in general_articles:\n",
    "        if article.my_id in target_myids:\n",
    "            removed_targets.append(article)\n",
    "            general_articles.remove(article)\n",
    "    for removed in removed_targets:\n",
    "        print(\"Removed target from pool: {}\".format(removed.title))\n",
    "    return general_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of articles and abstracts from general publications, if abstract is sufficiently long\n",
    "pool_articles = []\n",
    "pool_abstracts = []\n",
    "for article in general_articles:\n",
    "    abstract = article.abstract or ''\n",
    "    abstract = abstract.strip()\n",
    "    if len(abstract) > 50:\n",
    "        pool_articles.append(article)\n",
    "        pool_abstracts.append(abstract)\n",
    "print(\"Retained {} articles from {} general articles.\".format(len(pool_articles), len(general_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a list with all target abstracts, and list of all target articles in same order\n",
    "target_abstracts = []\n",
    "for article in target_articles:\n",
    "    target_abstracts.append(article.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a joint corpus\n",
    "all_corpus = pool_abstracts + target_abstracts\n",
    "print(\"Kept total of {} articles for NLP processing.\".format(len(all_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary for easier look-up of matched articles\n",
    "pool_articles_dict = {}\n",
    "for article in pool_articles:\n",
    "    pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define STOP words\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_abstract(abstract):\n",
    "    # lower case and remove special characters/whitespaces\n",
    "    abstract = re.sub(r'[^a-zA-Z0-9\\s]', '', abstract, re.I|re.A)\n",
    "    abstract = abstract.lower()\n",
    "    abstract = abstract.strip()\n",
    "    # tokanize\n",
    "    tokens = nltk.word_tokenize(abstract)\n",
    "    # filter stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in STOP]\n",
    "    # re-create text from filtered tokens\n",
    "    abstract = ' '.join(filtered_tokens)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_abstract)\n",
    "norm_corpus = normalize_corpus(all_corpus)\n",
    "print(\"Normalized {} articles.\".format(len(norm_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up TF-IDF representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# We take uni-gram and bi-grams as our features and remove terms \n",
    "# that occur only in one document across the whole corpus.\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity comparison (Cosine similarity for pairwise document similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target and pool tfidf\n",
    "target_tfidf = tfidf_matrix[-len(target_abstracts):]\n",
    "pool_tfidf = tfidf_matrix[:-len(target_abstracts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run full matrix similarity for pool vs target\n",
    "sim = pool_tfidf @ target_tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save similarity matrix as numpy object (natural break-point in work flow)\n",
    "# np.save(\"doc_sim.npy\", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy object: \n",
    "# sim = np.load(\"doc_sim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix\n",
    "coo_sim = sim.tocoo(copy=False)\n",
    "pool_idx = coo_sim.row\n",
    "target_idx = coo_sim.col\n",
    "flat_sim = coo_sim.data\n",
    "#free up some memory\n",
    "del tfidf_matrix, target_tfidf, pool_tfidf, sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for similarity threshold\n",
    "useful = np.argwhere(flat_sim > 0.13)\n",
    "filtered_pool_idx = pool_idx[useful].flatten()\n",
    "filtered_target_idx = target_idx[useful].flatten()\n",
    "filtered_flat_sim = flat_sim[useful].flatten()\n",
    "print(\"Identified {} articles above similarity threshold.\".format(len(useful)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(filtered_flat_sim)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_matches has all matches in order\n",
    "filtered_pool_idx = np.array(filtered_pool_idx, dtype=int)\n",
    "filtered_target_idx = np.array(filtered_target_idx, dtype=int)\n",
    "sorted_matches = []\n",
    "for i in order:\n",
    "    match = (filtered_flat_sim[i], pool_articles[filtered_pool_idx[i]], target_articles[filtered_target_idx[i]])\n",
    "    sorted_matches.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency table (how many matches does each pool article have)\n",
    "from collections import Counter\n",
    "pool_hits = Counter(filtered_pool_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles have at least X matches?\n",
    "sum([1 for x in pool_hits.values() if x >= 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the counter\n",
    "{x : pool_hits[x] for x in pool_hits if pool_hits[x] >= 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_matches = defaultdict(list) #keys are pool Article.my_id's, values are lists of matched target article obj\n",
    "for sim, pool, target in sorted_matches:\n",
    "    # create key; add similarity score; append a tuple that has matched target article and it\n",
    "    pool_matches[pool.my_id].append((sim, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out matches\n",
    "with open(\"./abstract_matches.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(pool_matches, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those with less than X matches\n",
    "# for my_id, match_list in list(pool_matches.items()):\n",
    "#     if len(match_list) < 2:\n",
    "#         pool_matches.pop(my_id)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_iter = iter(pool_matches.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_id, matches = next(match_iter)\n",
    "print(\"Pool article:\")\n",
    "print(pool_articles_dict[my_id].title)\n",
    "print(pool_articles_dict[my_id].my_id)\n",
    "print(pool_articles_dict[my_id].year)\n",
    "print(pool_articles_dict[my_id].abstract)\n",
    "\n",
    "for sim, jm in matches:\n",
    "    print()\n",
    "    print(jm.title, jm.my_id, jm.year, sim)\n",
    "    print(jm.abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matching articles based on reference similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17452 general articles.\n",
      "Read in 4 target articles.\n"
     ]
    }
   ],
   "source": [
    "# retrieve general articles with reference data\n",
    "with open(\"./plant_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    general_articles = pickle.load(infile)\n",
    "print(\"Read in {} general articles.\".format(len(general_articles)))\n",
    "# retrieve target articles\n",
    "with open(\"./my_publications/parsed_articles.pickle\", \"rb\") as infile:\n",
    "    target_articles = pickle.load(infile)\n",
    "print(\"Read in {} target articles.\".format(len(target_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed target from pool: Flagellin perception varies quantitatively in Arabidopsis thaliana and its relatives.\n"
     ]
    }
   ],
   "source": [
    "# Remove targets from general pool\n",
    "pool_articles = remove_targets_from_general(target_articles, general_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_articles_dict = {}\n",
    "# # build a dictionary for easier look-up of matched articles\n",
    "# for article in pool_articles:\n",
    "#     pool_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_articles_dict = {}\n",
    "# build a dictionary for easier look-up of matched articles\n",
    "for article in target_articles:\n",
    "    target_articles_dict[article.my_id] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 of 4 target articles have references.\n"
     ]
    }
   ],
   "source": [
    "target_temp = []\n",
    "for article in target_articles:\n",
    "    # check if the string references have been converted (to article objects)\n",
    "    if not any(isinstance(r, str) for r in article.references):\n",
    "        target_temp.append(article)\n",
    "print(\"{} of {} target articles have references.\".format(len(target_temp), len(target_articles)))\n",
    "# re-asign target_articles to remove target articles without reference information\n",
    "target_articles = target_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_ID = 100\n",
    "\n",
    "def get_reference_token(article):\n",
    "    global UNIQUE_ID\n",
    "    if article.doi:\n",
    "        return article.doi\n",
    "    elif article.title:\n",
    "        title = article.title.lower()\n",
    "        return re.sub(r'[^a-z0-9]', '', title)\n",
    "    else:\n",
    "        # NOTE: if you want to try matching just on the understandable references,\n",
    "        # you can instead return \"None\" here. (Expect more matches, but also more false positives)\n",
    "        UNIQUE_ID += 1\n",
    "        return \"LOCAL\" + str(UNIQUE_ID)\n",
    "\n",
    "def reference_tokenizer(article):\n",
    "    tokens = []\n",
    "    for ref in article.references:\n",
    "        token = get_reference_token(ref)\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 4 target articles with sufficient reference information.\n"
     ]
    }
   ],
   "source": [
    "n_target = 0 # number of useful target articles (i.e., 5+ refs)\n",
    "ref_texts = []\n",
    "ref_articles = []\n",
    "for article in pool_articles:\n",
    "    tokens = reference_tokenizer(article)\n",
    "    if tokens and len(tokens) > 5:\n",
    "        ref_texts.append(tokens)\n",
    "        ref_articles.append(article)\n",
    "for article in target_articles:\n",
    "    tokens = reference_tokenizer(article)\n",
    "    if tokens and len(tokens) > 5:\n",
    "        n_target += 1\n",
    "        ref_texts.append(tokens)\n",
    "        ref_articles.append(article)\n",
    "print(\"Identified {} target articles with sufficient reference information.\".format(n_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering of reference information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# We take uni-gram and bi-grams as our features and remove terms \n",
    "# that occur only in one document across the whole corpus.         <- is that smart?\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf_matrix = tf.fit_transform(ref_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cosine similarity matrix between pool and target articles\n",
    "pool_tfidf = tfidf_matrix[:-n_target]\n",
    "pool_articles = ref_articles[:-n_target]\n",
    "target_tfidf = tfidf_matrix[-n_target:]\n",
    "target_articles = ref_articles[-n_target:]\n",
    "sim = pool_tfidf @ target_tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all matching pairs of articles, in sorted order\n",
    "coo_sim = sim.tocoo(copy=False)\n",
    "pool_idx = coo_sim.row\n",
    "target_idx = coo_sim.col\n",
    "flat_sim = coo_sim.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory and order per similarity\n",
    "# del tfidf_matrix, target_tfidf, pool_tfidf, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# adjust stringency of matches by filtering for flat_sim (similarity) \n",
    "useful = np.argwhere(flat_sim > 0.01)\n",
    "filtered_pool_idx = pool_idx[useful].flatten()\n",
    "filtered_target_idx = target_idx[useful].flatten()\n",
    "filtered_flat_sim = flat_sim[useful].flatten()\n",
    "\n",
    "from collections import Counter\n",
    "target_hits = Counter(filtered_target_idx)\n",
    "# for reference... number of target articles with XXX matches in pool\n",
    "print(sum([1 for x in target_hits.values() if x >= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(filtered_flat_sim)[::-1]\n",
    "filtered_pool_idx = np.array(filtered_pool_idx, dtype=int)\n",
    "filtered_target_idx = np.array(filtered_target_idx, dtype=int)\n",
    "sorted_matches = []\n",
    "for i in order:\n",
    "    match = (filtered_flat_sim[i], target_articles[filtered_target_idx[i]], pool_articles[filtered_pool_idx[i]])\n",
    "    sorted_matches.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matches = defaultdict(list) #keys are target Article.my_id's, values are lists of matched pool article obj\n",
    "for sim, target, pool in sorted_matches:\n",
    "    # create key; add similarity score; append a tuple that has matched pool article and its sim score\n",
    "    target_matches[target.my_id].append((sim, pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out matches\n",
    "with open(\"./ref_matches.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(pool_matches, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if desired, remove those with less than X matches\n",
    "# for my_id, match_list in list(target_matches.items()):\n",
    "#     if len(match_list) < 4:\n",
    "#         target_matches.pop(my_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_iter = iter(target_matches.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target article:\n",
      "16Stimator: statistical estimation of ribosomal gene copy numbers from draft genome assemblies.\n",
      "10.1038/ismej.2015.161\n",
      "2016\n",
      "\n",
      "                The 16S rRNA gene (16S) is an accepted marker of bacterial taxonomic diversity, even though differences in copy number obscure the relationship between amplicon and organismal abundances. Ancestral state reconstruction methods can predict 16S copy numbers through comparisons with closely related reference genomes; however, the database of closed genomes is limited. Here, we extend the reference database of 16S copy numbers to de novo assembled draft genomes by developing 16Stimator, a method to estimate 16S copy numbers when these repetitive regions collapse during assembly. Using a read depth approach, we estimate 16S copy numbers for 12 endophytic isolates from Arabidopsis thaliana and confirm estimates by qPCR. We further apply this approach to draft genomes deposited in NCBI and demonstrate accurate copy number estimation regardless of sequencing platform, with an overall median deviation of 14%. The expanded database of isolates with 16S copy number estimates increases the power of phylogenetic correction methods for determining organismal abundances from 16S amplicon surveys. \n",
      "            \n",
      "            \n",
      "\n",
      "Title: Characterization of novel glycosyl hydrolases discovered by cell wall glycan directed monoclonal antibody screening and metagenome analysis of maize aerial root mucilage. \n",
      " my_id: 10.1371/journal.pone.0204525 \n",
      " Year: 2018 \n",
      " Similarity score: 0.04183852369828834 \n",
      " Abstract: \n",
      "                An indigenous maize landrace from the Sierra Mixe region of Oaxaca, Mexico exhibits extensive formation of aerial roots which exude large volumes of a polysaccharide-rich gel matrix or \"mucilage\" that harbors diazotrophic microbiota. We hypothesize that the mucilage associated microbial community carries out multiple functions, including disassembly of the mucilage polysaccharide. In situ, hydrolytic assay of the mucilage revealed endogenous arabinofuranosidase, galactosidase, fucosidase, mannosidase and xylanase activities. Screening the mucilage against plant cell wall glycan-specific monoclonal antibodies recognized the presence of carbohydrate epitopes of hemicellulosic polysaccharides like xyloglucan (both non-fucosylated and fucosylated), xylan (both substituted and unsubstituted xylan domains) and pectic-arabinogalactans, all of which are potential carbon sources for mucilage microbial residents. Mucilage metagenome annotation using MG-RAST identified the members forming the microbial community, and gene fragments with predicted functions associated with carbohydrate disassembly. Data from the in situ hydrolytic activity and monoclonal antibody screening assays were used to guide the selection of five full length genes with predicted glycosyl hydrolase function from the GenBank database that were similar to gene fragments of high relative abundance in the mucilage metagenomes. These five genes were then synthesized for recombinant production in Escherichia coli. Here we report the characterization of an α-N-arabinofuranosidase (GH51) and an oligosaccharide reducing-end xylanase (GH8) from Flavobacterium johnsoniae; an α-L-fucosidase (GH29) and a xylan β-1,4 xylosidase (GH39) from Spirosoma linguale, and a β-mannosidase (GH2) from Agrobacterium fabrum. Biochemical characterization of these enzymes revealed a β-Mannosidase that also exhibits a secondary activity towards the cleavage of galactosyl residues. We also describe two xylanases (GH8 and GH39) from underexplored glycosyl hydrolase families, one thermostable α-L-Fucosidase (GH29) and a thermostable α-N-Arabinofuranosidase (GH51).\n",
      "            \n",
      "            \n",
      "\n",
      "Title: A gut bacterial pathway metabolizes aromatic amino acids into nine circulating metabolites. \n",
      " my_id: 10.1038/nature24661 \n",
      " Year: 2017 \n",
      " Similarity score: 0.04059552072382363 \n",
      " Abstract: \n",
      "                The human gut microbiota produces dozens of metabolites that accumulate in the bloodstream, where they can have systemic effects on the host. Although these small molecules commonly reach concentrations similar to those achieved by pharmaceutical agents, remarkably little is known about the microbial metabolic pathways that produce them. Here we use a combination of genetics and metabolic profiling to characterize a pathway from the gut symbiont Clostridium sporogenes that generates aromatic amino acid metabolites. Our results reveal that this pathway produces twelve compounds, nine of which are known to accumulate in host serum. All three aromatic amino acids (tryptophan, phenylalanine and tyrosine) serve as substrates for the pathway, and it involves branching and alternative reductases for specific intermediates. By genetically manipulating C. sporogenes, we modulate serum levels of these metabolites in gnotobiotic mice, and show that in turn this affects intestinal permeability and systemic immunity. This work has the potential to provide the basis of a systematic effort to engineer the molecular output of the gut bacterial community.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: wblE2 transcription factor in Streptomyces griseus S4-7 plays an important role in plant protection. \n",
      " my_id: 10.1002/mbo3.494 \n",
      " Year: 2017 \n",
      " Similarity score: 0.03825381411411317 \n",
      " Abstract: \n",
      "                Streptomyces griseus S4-7 was originally isolated from the strawberry rhizosphere as a microbial agent responsible for Fusarium wilt suppressive soils. S. griseus S4-7 shows specific and pronounced antifungal activity against Fusarium oxysporum f. sp. fragariae. In the Streptomyces genus, the whi transcription factors are regulators of sporulation, cell differentiation, septation, and secondary metabolites production. wblE2 function as a regulator has emerged as a new group in whi transcription factors. In this study, we reveal the involvement of the wblE2 transcription factor in the plant-protection by S. griseus S4-7. We generated ΔwblE, ΔwblE2, ΔwhiH, and ΔwhmD gene knock-out mutants, which showed less antifungal activity both in vitro and in planta. Among the mutants, wblE2 mutant failed to protect the strawberry against the Fusarium wilt pathogen. Transcriptome analyses revealed major differences in the regulation of phenylalanine metabolism, polyketide and siderophore biosynthesis between the S4-7 and the wblE2 mutant. The results contribute to our understanding of the role of streptomycetes wblE2 genes in a natural disease suppressing system.\n",
      "                © 2017 The Authors. MicrobiologyOpen published by John Wiley & Sons Ltd.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Genome-wide patterns of genetic variation in sweet and grain sorghum (Sorghum bicolor). \n",
      " my_id: 10.1186/gb-2011-12-11-r114 \n",
      " Year: 2011 \n",
      " Similarity score: 0.0370783786450945 \n",
      " Abstract: \n",
      "                Sorghum (Sorghum bicolor) is globally produced as a source of food, feed, fiber and fuel. Grain and sweet sorghums differ in a number of important traits, including stem sugar and juice accumulation, plant height as well as grain and biomass production. The first whole genome sequence of a grain sorghum is available, but additional genome sequences are required to study genome-wide and intraspecific variation for dissecting the genetic basis of these important traits and for tailor-designed breeding of this important C4 crop.\n",
      "                We resequenced two sweet and one grain sorghum inbred lines, and identified a set of nearly 1,500 genes differentiating sweet and grain sorghum. These genes fall into ten major metabolic pathways involved in sugar and starch metabolisms, lignin and coumarin biosynthesis, nucleic acid metabolism, stress responses and DNA damage repair. In addition, we uncovered 1,057,018 SNPs, 99,948 indels of 1 to 10 bp in length and 16,487 presence/absence variations as well as 17,111 copy number variations. The majority of the large-effect SNPs, indels and presence/absence variations resided in the genes containing leucine rich repeats, PPR repeats and disease resistance R genes possessing diverse biological functions or under diversifying selection, but were absent in genes that are essential for life.\n",
      "                This is a first report of the identification of genome-wide patterns of genetic variation in sorghum. High-density SNP and indel markers reported here will be a valuable resource for future gene-phenotype studies and the molecular breeding of this important crop and related species.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Harnessing the microbiomes of Brassica vegetables for health issues. \n",
      " my_id: 10.1038/s41598-017-17949-z \n",
      " Year: 2017 \n",
      " Similarity score: 0.029401573064829146 \n",
      " Abstract: \n",
      "                Plant health is strongly connected with plants´ microbiome. In case of raw-eaten plants, the microbiome can also affect human health. To study potential impacts on health issues of both hosts, the microbiome composition of seven different Brassica vegetables, originating from different food processing pathways, was analyzed by a combined approach of amplicon sequencing, metagenomic mining and cultivation. All Brassica vegetables harbored a highly diverse microbiota as identified by 16S rRNA gene amplicon sequencing. The composition of the microbiota was found to be rather driven by the plant genotype than by the processing pathway. We characterized isolates with potential cancer-preventing properties by tracing myrosinase activity as well as isolates with biological control activity towards plant pathogens. We identified a novel strain with myrosinase activity and we found bacterial myrosinase genes to be enriched in rhizosphere and phyllosphere metagenomes of Brassica napus and Eruca sativa in comparison to the surrounding soil. Strains which were able to suppress plant pathogens were isolated from naturally processed vegetables and represent a substantial part (4.1%) of all vegetable microbiomes. Our results shed first light on the microbiome of edible plants and open the door to harnessing the Brassica microbiome for plant disease resistance and human health.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Identification of QTLs controlling resistance to Pseudomonas syringae pv. tomato race 1 strains from the wild tomato, Solanum habrochaites LA1777. \n",
      " my_id: 10.1007/s00122-015-2463-7 \n",
      " Year: 2015 \n",
      " Similarity score: 0.026385288075194215 \n",
      " Abstract: \n",
      "                Screening of wild tomato accessions revealed a source of resistance to Pseudomonas syringe pv. tomato race 1 from Solanum habrochaites and facilitated mapping of QTLs controlling disease resistance. Pseudomonas syringae pv. tomato (Pst) causes bacterial speck of tomato, which is one of the most persistent bacterial diseases in tomato worldwide. Existing Pst populations have overcome genetic resistance mediated by the tomato genes Pto and Prf. The objective of this study was to identify sources of resistance to race 1 strains and map quantitative trait loci (QTLs) controlling resistance in the wild tomato Solanum habrochaites LA1777. Pst strains A9 and 407 are closely related to current field strains and genome sequencing revealed the lack of the avrPto effector as well as select mutations in the avrPtoB effector, which are recognized by Pto and Prf. Strains A9 and 407 were used to screen 278 tomato accessions, identifying five exhibiting resistance: S. peruvianum LA3799, S. peruvianum var. dentatum PI128655, S. chilense LA2765, S. habrochaites LA2869, and S. habrochaites LA1777. An existing set of 93 introgression lines developed from S. habrochaites LA1777 was screened for resistance to strain A9 in a replicated greenhouse trial. Four QTLs were identified using composite interval mapping and mapped to different chromosomes. bsRr1-1 was located on chromosome 1, bsRr1-2 on chromosome 2, and bsRr1-12a and bsRr1-12b on chromosome 12. The QTLs detected explained 10.5-12.5% of the phenotypic variation. Promising lines were also subjected to bacterial growth curves to verify resistance and were analyzed for general horticultural attributes under greenhouse conditions. These findings will provide useful information for future high-resolution mapping of each QTL and integration into marker-assisted breeding programs.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Dynamic Alterations in the Gut Microbiota of Collagen-Induced Arthritis Rats Following the Prolonged Administration of Total Glucosides of Paeony. \n",
      " my_id: 10.3389/fcimb.2019.00204 \n",
      " Year: 2019 \n",
      " Similarity score: 0.02620848019401228 \n",
      " Abstract: \n",
      "                Rheumatoid arthritis (RA) is a common autoimmune disease linked to chronic inflammation. Dysbiosis of the gut microbiota has been proposed to contribute to the risk of RA, and a large number of researchers have investigated the gut-joint axis hypothesis using the collagen-induced arthritis (CIA) rats. However, previous studies mainly involved short-term experiments; very few used the CIA model to investigate changes in gut microbiota over time. Moreover, previous research failed to use the CIA model to carry out detailed investigations of the effects of drug treatments upon inflammation in the joints, hyperplasia of the synovium, imbalance in the ratios of Th1/Th2 and Th17/Treg cells, intestinal cytokines and the gut microbiota following long-term intervention. In the present study, we carried out a 16-week experiment to investigate changes in the gut microbiota of CIA rats, and evaluated the modulatory effect of total glucosides of paeony (TGP), an immunomodulatory agent widely used in the treatment of RA, after 12 weeks of administration. We found that taxonomic differences developed in the microbial structure between the CIA group and the Control group. Furthermore, the administration of TGP was able to correct 78% of these taxonomic differences, while also increase the relative abundance of certain forms of beneficial symbiotic bacteria. By the end of the experiment, TGP had reduced body weight, thymus index and inflammatory cell infiltration in the ankle joint of CIA rats. Furthermore, the administration of TGP had down-regulated the synovial content of VEGF and the levels of Th1 cells and Th17 cells in CIA rats, and up-regulated the levels of Th2 cells and Treg cells. The administration of TGP also inhibited the levels of intestinal cytokines, secretory immunoglobulin A (SIgA) and Interferon-γ (IFN-γ). In conclusion, the influence of TGP on dynamic changes in gut microbiota, along with the observed improvement of indicators related to CIA symptoms during 12 weeks of administration, supported the hypothesis that the microbiome may play a role in TGP-mediated therapeutic effects in CIA rats. The present study also indicated that the mechanism underlying these effects may be related to the regulation of intestinal mucosal immunity remains unknown and deserves further research attention.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Enhanced resistance to blister blight in transgenic tea (Camellia sinensis [L.] O. Kuntze) by overexpression of class I chitinase gene from potato (Solanum tuberosum). \n",
      " my_id: 10.1007/s10142-015-0436-1 \n",
      " Year: 2015 \n",
      " Similarity score: 0.025822205331878317 \n",
      " Abstract: \n",
      "                Tea is the second most consumed beverage in the world. A crop loss of up to 43 % has been reported due to blister blight disease of tea caused by a fungus, Exobasidium vexans. Thus, it directly affects the tea industry qualitatively and quantitatively. Solanum tuberosum class I chitinase gene (AF153195) is a plant pathogenesis-related gene. It was introduced into tea genome via Agrobacterium-mediated transformation with hygromycin phosphotransferase (hpt) gene conferring hygromycin resistance as plant selectable marker. A total of 41 hygromycin resistant plantlets were obtained, and PCR analysis established 12 plantlets confirming about the stable integration of transgene in the plant genome. Real-time PCR detected transgene expression in four transgenic plantlets (T28, C57, C9, and T31). Resistance to biotrophic fungal pathogen, E. vexans, was tested by detached leaf infection assay of greenhouse acclimated plantlets. An inhibitory activity against the fungal pathogen was evident from the detached leaves from the transformants compared with the control. Fungal lesion formed on control plantlet whereas the transgenic plantlets showed resistance to inoculated fungal pathogen by the formation of hypersensitivity reaction area. This result suggests that constitutive expression of the potato class I chitinase gene can be exploited to improve resistance to fungal pathogen, E. vexans, in economical perennial plantation crop like tea.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Influence of resistance breeding in common bean on rhizosphere microbiome composition and function. \n",
      " my_id: 10.1038/ismej.2017.158 \n",
      " Year: 2018 \n",
      " Similarity score: 0.023298376135648044 \n",
      " Abstract: \n",
      "                The rhizosphere microbiome has a key role in plant growth and health, providing a first line of defense against root infections by soil-borne pathogens. Here, we investigated the composition and metabolic potential of the rhizobacterial community of different common bean (Phaseolus vulgaris) cultivars with variable levels of resistance to the fungal root pathogen Fusarium oxysporum (Fox). For the different bean cultivars grown in two soils with contrasting physicochemical properties and microbial diversity, rhizobacterial abundance was positively correlated with Fox resistance. Pseudomonadaceae, bacillaceae, solibacteraceae and cytophagaceae were more abundant in the rhizosphere of the Fox-resistant cultivar. Network analyses showed a modular topology of the rhizosphere microbiome of the Fox-resistant cultivar, suggesting a more complex and highly connected bacterial community than in the rhizosphere of the Fox-susceptible cultivar. Metagenome analyses further revealed that specific functional traits such as protein secretion systems and biosynthesis genes of antifungal phenazines and rhamnolipids were more abundant in the rhizobacterial community of the Fox-resistant cultivar. Our findings suggest that breeding for Fox resistance in common bean may have co-selected for other unknown plant traits that support a higher abundance of specific beneficial bacterial families in the rhizosphere with functional traits that reinforce the first line of defense.\n",
      "            \n",
      "            \n",
      "\n",
      "Title: Copy number variation and disease resistance in plants. \n",
      " my_id: 10.1007/s00122-017-2993-2 \n",
      " Year: 2017 \n",
      " Similarity score: 0.022167634640868527 \n",
      " Abstract: \n",
      "                Plant genome diversity varies from single nucleotide polymorphisms to large-scale deletions, insertions, duplications, or re-arrangements. These re-arrangements of sequences resulting from duplication, gains or losses of DNA segments are termed copy number variations (CNVs). During the last decade, numerous studies have emphasized the importance of CNVs as a factor affecting human phenotype; in particular, CNVs have been associated with risks for several severe diseases. In plants, the exploration of the extent and role of CNVs in resistance against pathogens and pests is just beginning. Since CNVs are likely to be associated with disease resistance in plants, an understanding of the distribution of CNVs could assist in the identification of novel plant disease-resistance genes. In this paper, we review existing information about CNVs; their importance, role and function, as well as their association with disease resistance in plants.\n",
      "            \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# step through the results by re-running this cell multiple times\n",
    "my_id, matches = next(match_iter)\n",
    "print(\"Target article:\")\n",
    "print(target_articles_dict[my_id].title)\n",
    "print(target_articles_dict[my_id].my_id)\n",
    "print(target_articles_dict[my_id].year)\n",
    "print(target_articles_dict[my_id].abstract)\n",
    "\n",
    "for sim, pool_match in matches:\n",
    "    print()\n",
    "    print(\"Title: {} \\n my_id: {} \\n Year: {} \\n Similarity score: {} \\n Abstract: {}\".format(pool_match.title, pool_match.my_id, pool_match.year, sim, pool_match.abstract))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
